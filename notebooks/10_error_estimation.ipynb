{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c70fa63-b796-4b94-b670-0e98bb66a5a1",
   "metadata": {},
   "source": [
    "# Error Estimation\n",
    "\n",
    "In this notebook, we will implement and test **error estimation approaches** for evaluating classifiers and/or learning algorithms.\n",
    "\n",
    "At the start, we will implement the $k$-fold cross-validation with and without stratification.\n",
    "\n",
    "Subsequently, we will use the nested $k$-fold cross-validation on an exemplary dataset to perform model selection.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [$k$-fold Cross-alidation](#k-fold-cross-validation)\n",
    "2. [Model Selection](#model-selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb5690-6d49-42df-bf4b-db120f76c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee2541",
   "metadata": {},
   "source": [
    "### **1. $k$-fold Cross-validation** <a class=\"anchor\" id=\"k-fold-cross-validation\"></a>\n",
    "\n",
    "We implement the function [`cross_validation`](../e2ml/evaluation/_error_estimation.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we visualize and compare the standard and stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import cross_validation\n",
    "# Generate articial class labels.\n",
    "y = np.zeros(100)\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "y[30:90] = 1\n",
    "y[90:] = 2\n",
    "\n",
    "# Visualize standard (k=3)-fold cross validation via a bar plot showing the\n",
    "# class distribution within each fold.\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices, n_folds=3, random_state=0\n",
    ")\n",
    "class_distribution = []\n",
    "plt.title(\"Standard $k=3$-fold Cross-valdiation\")\n",
    "for i, class_y in enumerate(np.unique(y)):\n",
    "    y_values = []\n",
    "    for t in test:\n",
    "        y_values.append(np.sum(y[t] == class_y))\n",
    "    class_distribution.append(y_values)\n",
    "    plt.bar(np.arange(len(test)), y_values, bottom=np.sum(np.array(class_distribution)[:i], axis=0))\n",
    "    plt.xticks([0, 1, 2])\n",
    "plt.show()\n",
    "\n",
    "# END SOLUTION\n",
    "    \n",
    "# Visualize stratified (k=3)-fold cross validation via a bar plot showing the class\n",
    "# distribution within each fold.\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices, n_folds=3, random_state=0, y=y\n",
    ")\n",
    "class_distribution = []\n",
    "plt.title(\"Stratified $k=3$-fold Cross-valdiation\")\n",
    "for i, class_y in enumerate(np.unique(y)):\n",
    "    y_values = []\n",
    "    for t in test:\n",
    "        y_values.append(np.sum(y[t] == class_y))\n",
    "    class_distribution.append(y_values)\n",
    "    plt.bar(np.arange(len(test)), y_values, bottom=np.sum(np.array(class_distribution)[:i], axis=0))\n",
    "    plt.xticks([0, 1, 2])\n",
    "plt.show()\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e4210",
   "metadata": {},
   "source": [
    "### **2. Model Selection** <a class=\"anchor\" id=\"model-selection\"></a>\n",
    "\n",
    "In the follwing, we perform a small evaluation study including a model selection. Our goal is to compare the learning algorithm of a [*support vector classifier*](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (SVC) and a [*multi-layer perceptron*](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) (MLP) on the data set [*breast cancer*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer). We generate in each run 20 hyperparameter configurations according to one of the popular experimentation methods. Studied hyperparamters are the regularization parameter $C \\in (0, 1000)$ (`C`) and the so-called bandwidth $\\gamma \\in (0, 1]$ (`gamma`) for the SVC, while the learning rate $\\eta \\in (0, 1]$ (`learning_rate_init`) and another regularization parameter $\\alpha \\in (0, 1)$ (`alpha`) are studied for the MLP. Further, we use a nested stratified $k=5$-folded cross-valdiation as error-estimation approach. The zero-one loss serves as performance measure to report the emprical mean and standard deviation of the risk estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e73d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load breast cancer data set.\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Perform evaluation study.\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "from e2ml.experimentation import halton\n",
    "from e2ml.preprocessing import StandardScaler\n",
    "from e2ml.evaluation import zero_one_loss\n",
    "\n",
    "# Define number of folds.\n",
    "n_folds = 5\n",
    "\n",
    "# Placeholder for emprical risks of SVC and MLP per fold in the outer loop.\n",
    "risks_svc_outer, risks_mlp_outer = [], []\n",
    "\n",
    "# Create hyperparamter configuration using the halton sequence.\n",
    "theta_svc_list = halton(n_samples=20, n_dimensions=2, bounds=[(0, 1000), (0, 1)])\n",
    "theta_mlp_list = halton(n_samples=20, n_dimensions=2, bounds=[(0, 1), (0, 1)])\n",
    "\n",
    "# Perform $k$-fold cross validation as outer loop.\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "train_outer, test_outer = cross_validation(\n",
    "    sample_indices=sample_indices, n_folds=n_folds, y=y, random_state=0\n",
    ")\n",
    "for tr_outer, te_outer in zip(train_outer, test_outer):\n",
    "    # Perform $k$-fold cross-validation as inner loop.\n",
    "    train_inner, test_inner = cross_validation(\n",
    "        sample_indices=tr_outer, n_folds=n_folds, y=y[tr_outer], random_state=0\n",
    "    )\n",
    "    \n",
    "    # Define inital best hyperparameters and risk estimates.\n",
    "    theta_star_svc, theta_star_mlp = None, None\n",
    "    minimum_risk_svc, minimum_risk_mlp = 1, 1\n",
    "    \n",
    "    # Perform model selection.\n",
    "    for theta_svc, theta_mlp in zip(theta_svc_list, theta_mlp_list):\n",
    "        # Placeholder for emprical risks of SVC and MLP per fold in the inner loop.\n",
    "        risks_svc_inner, risks_mlp_inner = [], []\n",
    "        \n",
    "        for tr_inner, te_inner in zip(train_inner, test_inner):\n",
    "            # Standardize data.\n",
    "            sc_inner = StandardScaler().fit(X[tr_inner])\n",
    "            X_tr_inner = sc_inner.transform(X[tr_inner]) \n",
    "            X_te_inner = sc_inner.transform(X[te_inner])\n",
    "            \n",
    "            # Fit SVC in the inner loop.\n",
    "            svc_inner = SVC(\n",
    "                C=theta_svc[0],\n",
    "                gamma=theta_svc[1],\n",
    "                random_state=0\n",
    "            )\n",
    "            svc_inner.fit(X_tr_inner, y[tr_inner])\n",
    "            \n",
    "            # Evaluate SVC in the inner loop.\n",
    "            y_pred = svc_inner.predict(X_te_inner)\n",
    "            risks_svc_inner.append(\n",
    "                zero_one_loss(y_pred=y_pred, y_true=y[te_inner])\n",
    "            )\n",
    "            \n",
    "            # Fit MLP in the inner loop.\n",
    "            mlp_inner = MLPClassifier(\n",
    "                learning_rate_init=theta_mlp[0],\n",
    "                alpha=theta_mlp[1],\n",
    "                random_state=0\n",
    "            )\n",
    "            mlp_inner.fit(X_tr_inner, y[tr_inner])\n",
    "            \n",
    "            # Evaluate MLP in the inner loop.\n",
    "            y_pred = mlp_inner.predict(X_te_inner)\n",
    "            risks_mlp_inner.append(\n",
    "                zero_one_loss(y_pred=y_pred, y_true=y[te_inner])\n",
    "            )\n",
    "            \n",
    "        # Update best hyperparamter configuration for SVC.\n",
    "        if np.mean(risks_svc_inner) <= minimum_risk_svc:\n",
    "            theta_star_svc = theta_svc\n",
    "            minimum_risk_svc = np.mean(risks_svc_inner)\n",
    "            \n",
    "        # Update best hyperparamter configuration for MLP.\n",
    "        if np.mean(risks_mlp_inner) <= minimum_risk_mlp:\n",
    "            theta_star_mlp = theta_mlp\n",
    "            minimum_risk_mlp = np.mean(risks_mlp_inner)\n",
    "    \n",
    "    # Standardize data in the outer loop.\n",
    "    sc_outer = StandardScaler().fit(X[tr_outer])\n",
    "    X_tr_outer = sc_outer.transform(X[tr_outer]) \n",
    "    X_te_outer = sc_outer.transform(X[te_outer])\n",
    "\n",
    "    # Fit SVC in the outer loop.\n",
    "    svc_outer = SVC(\n",
    "        C=theta_star_svc[0],\n",
    "        gamma=theta_star_svc[1],\n",
    "        random_state=0\n",
    "    )\n",
    "    svc_outer.fit(X_tr_outer, y[tr_outer])\n",
    "\n",
    "    # Evaluate SVC in the outer loop.\n",
    "    y_pred = svc_outer.predict(X_te_outer)\n",
    "    risks_svc_outer.append(\n",
    "        zero_one_loss(y_pred=y_pred, y_true=y[te_outer])\n",
    "    )\n",
    "    \n",
    "    # Fit MLP in the outer loop.\n",
    "    mlp_outer = MLPClassifier(\n",
    "        learning_rate_init=theta_star_mlp[0],\n",
    "        alpha=theta_star_mlp[1], \n",
    "        random_state=0\n",
    "    )\n",
    "    mlp_outer.fit(X_tr_outer, y[tr_outer])\n",
    "\n",
    "    # Evaluate MLP in the outer loop.\n",
    "    y_pred = mlp_outer.predict(X_te_outer)\n",
    "    risks_mlp_outer.append(\n",
    "        zero_one_loss(y_pred=y_pred, y_true=y[te_outer])\n",
    "    )\n",
    "    \n",
    "print(f\"SVC: {np.mean(risks_svc_outer)} +- {np.std(risks_svc_outer)}\")\n",
    "print(f\"MLP: {np.mean(risks_mlp_outer)} +- {np.std(risks_mlp_outer)}\")\n",
    "    \n",
    "# END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
