{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "In this notebook, we will learn about the basics of data preprocessing. \n",
    "\n",
    "At the start, we will generate a toy **data set** and download an image data set.\n",
    "\n",
    "Subsequently, we will **standardize** both data sets.\n",
    "\n",
    "In a next step, we will implement the **principal component analysis** (PCA) algorithm and apply it on the artificial data set.\n",
    "\n",
    "Finally, we will test the performance of the PCA on the image data set.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Data Sets](#data-sets)\n",
    "2. [Standardization](#standardization)\n",
    "3. [PCA on Toy Data Set](#pca-on-toy-data-set)\n",
    "4. [PCA on Image Data Set](#pca-on-image-data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, Button, fixed\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Sets** <a class=\"anchor\" id=\"data-sets\"></a>\n",
    "\n",
    "In this notebook, we will consider two kinds of data sets:\n",
    "- a two-dimensional toy data set to illustrate eigenvectors\n",
    "- and the image data set [MNIST](https://www.openml.org/d/554) containing images of hand-written digits.\n",
    "\n",
    "Mathematically, we denote an obtained data set consisting of $N$ instances (also referred to as sample) as a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$, where the $n$-th row of this matrix represents the $n$-th instance being a $D$-dimensional feature vector: $\\mathbf{x}_n = (x_{n1}, \\dots, x_{nD})^\\mathrm{T}$.\n",
    "\n",
    "Below, we generate the toy data set, download the image data set, and visualize both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 samples using a 2-dimensional multivariate normal distributions with a mean=[5, 8] and\n",
    "# a covariance matrix cov=[[2, 1.9], [1.9, 2]]\n",
    "mean = [5, 8]\n",
    "cov = [[2, 1.9], [1.9, 2]]\n",
    "# TODO\n",
    "\n",
    "# Download MNIST data set.\n",
    "X_img, y_img = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_img, y_img = X_img.values, y_img.values.astype(int)\n",
    "\n",
    "# Plot data sets.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle('Visualization of Data Sets')\n",
    "ax[0].scatter(X_toy[:, 0], X_toy[:, 1])\n",
    "ax[0].set_title('Samples of Toy Data Set', fontsize=10)\n",
    "ax[1].imshow(X_img[0].reshape(28, 28), cmap='gray')\n",
    "ax[1].set_title('Image Sample of MNIST Data Set', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Standardization** <a class=\"anchor\" id=\"standardization\"></a>\n",
    "\n",
    "Before we implement PCA, we will need to standardize the data sets. The feature value $x_{*d}$ of an instance $\\mathbf{x}_*$ can be standardized according to\n",
    "\n",
    "TODO\n",
    "\n",
    "The standardization ensures that the instances will have zero mean and one variance.\n",
    "\n",
    "We need to implement the corresponding class [`StandardScaler`](../e2ml/preprocessing/_standard_scaler.py) in the [`e2ml.preprocessing`](../e2ml/preprocessing) subpackage.\n",
    "Once, the implementation has been completed, we actually standardize both data sets and visualize the results, and check our implementation's validity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.preprocessing import StandardScaler\n",
    "\n",
    "# Fit StandardScaler on the toy data set.\n",
    "# TODO\n",
    "\n",
    "# Use scaler to standardize the toy data set.\n",
    "# TODO\n",
    "\n",
    "assert np.allclose(np.std(X_toy_std, axis=0), np.ones(X_toy_std.shape[1])), 'The standard deviations are not one.'\n",
    "assert np.allclose(np.mean(X_toy_std, axis=0), np.zeros(X_toy_std.shape[1])), 'The means are not zero.'\n",
    "\n",
    "# Fit StandardScaler on the MNIST data set.\n",
    "# TODO\n",
    "\n",
    "# Fit StandardScaler on the MNIST data set.\n",
    "# TODO\n",
    "\n",
    "# Plot data sets.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle('Visualization of Standardized Data Sets', fontsize=16, y=1)\n",
    "ax[0].scatter(X_toy_std[:, 0], X_toy_std[:, 1])\n",
    "ax[0].set_title('Standardized Instances of Toy Data Set', fontsize=14)\n",
    "ax[1].imshow(X_img_std[0].reshape(28, 28), cmap='gray')\n",
    "ax[1].set_title('Standardized Image Instance of MNIST Data Set', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question:**\n",
    "2. (a) Why should we standardize data before applying PCA?\n",
    "\n",
    "   TODO\n",
    "\n",
    "### **3. PCA on Toy Data Set** <a class=\"anchor\" id=\"pca-on-toy-data-set\"></a>\n",
    "\n",
    "Now we will implement PCA given a training set $\\mathbf{X}$ with mean $\\mathbf{\\overline{x}} \\in \\mathbb{R}^D$ (being $\\mathbf{0}$ if $\\mathbf{X}$ has been standardized) to transform samples from the $D$-dimensional space into an $M \\leq D$ dimensional space. \n",
    "We then need to perform the following steps.\n",
    "\n",
    "1. Compute the (biased) covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D \\times D}_{\\geq 0}$:\n",
    "   \\begin{equation}\n",
    "   \\boldsymbol{\\Sigma} = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}_n-\\mathbf{\\overline{x}}) (\\mathbf{x}_n-\\mathbf{\\overline{x}})^\\mathrm{T}.\n",
    "   \\end{equation}\n",
    "2. Determine the eigenvector matrix $\\mathbf{U} \\in \\mathbb{R}^{D \\times D}$ of $\\boldsymbol{\\Sigma}$ including their corresponding eigenvalues $\\boldsymbol{\\lambda} \\in \\mathbb{R}^D$:\n",
    "    \\begin{equation}\n",
    "       \\boldsymbol{\\Sigma}\\mathbf{u}_i = \\lambda_i\\mathbf{u}_i,\n",
    "    \\end{equation}\n",
    "    where $\\mathbf{u}_i$ is the $i$-th column of the matrix $\\mathbf{U}$ and $\\lambda_i$ its eigenvalue.\n",
    "3. After these steps, we can then compute the projection matrix $\\mathbf{B} \\in \\mathbb{R}^{D \\times M}$ by selecting the top $M$ eigenvectors:\n",
    "    \\begin{equation}\n",
    "       \\mathbf{B} = \\begin{bmatrix} \\mathbf{u}_{i_1}, \\dots, \\mathbf{u}_{i_M} \\end{bmatrix} \\text{ subject to } \\lambda_{i_1} \\geq \\dots \\geq \\lambda_{i_M} \\text{ and } \\lambda_{i_M} \\geq \\lambda_j \\text{ for all } j \\in \\{1, \\dots, D\\} \\setminus \\{i_1, \\dots, i_M\\}.\n",
    "    \\end{equation}\n",
    "4. Finally, we can transform a sample $\\mathbf{x}_*$ according to\n",
    "   \\begin{equation}\n",
    "   \\mathbf{z}_{*} = \\mathbf{B}^\\mathrm{T}(\\mathbf{x}_*-\\mathbf{\\overline{x}}),\n",
    "   \\end{equation}\n",
    "   and retransform it to the original space through\n",
    "   \\begin{equation}\n",
    "   \\tilde{\\mathbf{x}}_{*} = \\mathbf{B}\\mathbf{z}_* + \\mathbf{\\overline{x}}.\n",
    "   \\end{equation}\n",
    "   \n",
    "A common way to determine the number of principal components $M$ is to define a threshold $c \\in (0, 1]$ for the **explained variance** such that \n",
    "\\begin{equation}\n",
    "M_c = \\text{arg min}_{M \\in \\{1, \\dots, D\\}} (M) \\text{ subject to } \\frac{\\sum_{j=1}^{M} \\lambda_{i_j}}{\\sum_{i=l}^{D} \\lambda_{l}} \\geq c.\n",
    "\\end{equation}\n",
    "\n",
    "   \n",
    "#### **Question:**\n",
    "3. (a) In which case is the retransformation $\\tilde{\\mathbf{x}}_{*}$ according to step 4 equal to $\\mathbf{x}_{*}$. We assume that all $D$ eigenvalues  in $\\boldsymbol{\\lambda}$ are positive?\n",
    "\n",
    "   TODO\n",
    "   \n",
    "Having implemented the class [`PrincipalComponentAnalysis`](../e2ml/preprocessing/_principal_component_analysis.py) in the [`e2ml.preprocessing`](../e2ml/preprocessing) subpackage, we apply it to the two-dimensional toy data set to visualize eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.preprocessing import PrincipalComponentAnalysis\n",
    "\n",
    "# Fit PrincipalComponentAnalysis object with M=1.\n",
    "# TODO\n",
    "\n",
    "# Transform instances of standardized toy data set.\n",
    "# TODO\n",
    "\n",
    "# Reconstruct original instances from instances samples.\n",
    "# TODO\n",
    "    \n",
    "# Visualize results of PCA.\n",
    "plt.figure()\n",
    "plt.suptitle(\"PCA Applied to Toy Data Set\", fontsize=16)\n",
    "plt.scatter(X_toy_std[:, 0], X_toy_std[:, 1], label='original instances')\n",
    "plt.scatter(X_rec[:, 0], X_rec[:, 1], label='reconstructed instances')\n",
    "for lmbda, u in zip(pca.lmbdas_, pca.U_.T):\n",
    "    u = u * 3 * np.sqrt(lmbda)\n",
    "    plt.annotate('', pca.mu_ + u, pca.mu_, arrowprops=dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "plt.axis('equal')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. PCA on Image Data Set** <a class=\"anchor\" id=\"pca-on-image-data-set\"></a>\n",
    "\n",
    "In this section, we will analyze the performance of the PCA on the image data set MNIST. Therefor, we analyze how the number of selected principal components $M$ affects our reconstruction error be.\n",
    "First, we have to implement a function for computing the reconstruction error (i.e., mean squared error):\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{N} \\sum_{n=1}^{N} ||\\mathbf{x}_n - \\tilde{\\mathbf{x}}_{n}||^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_error(X_orig, X_rec):\n",
    "    \"\"\"Computes the reconstruction error, i.e., mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_orig : array-like, shape (N, D)\n",
    "        Original samples.\n",
    "    X_rec : array-like, shape (N, D)\n",
    "        Reconstructed samples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        Reconstruction error.\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a second step, we will now compute the error in dependence of the number of selected components $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `M_list` as array of M's ranging from 1 to 50.\n",
    "# TODO\n",
    "\n",
    "# Iterate over different numbers of principal components M, compute the \n",
    "# reconstruction errors, and save them in `J_list`.\n",
    "# TODO\n",
    "    \n",
    "# Plot reconstruction error.\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(M_list, J_list)\n",
    "ax.xaxis.set_ticks(M_list)\n",
    "ax.axhline(xmin=0, xmax=1, linestyle='--', color='r', linewidth=2)\n",
    "ax.set_xlabel('number of principal components $M$', fontsize=14)\n",
    "ax.set_ylabel('reconstruction error $J$', fontsize=14)\n",
    "ax.set_title('Reconstruction Error vs. Number of Principal Components', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question:**\n",
    "4. (a) How can we use the eigenvalues for computing the reconstruction error on the training set?\n",
    "\n",
    "   TODO\n",
    "   \n",
    "Finally, we show the qualitative performance of the PCA through visual inspections of examples for different values of the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, len(X_img_std)-1), x0=fixed(28), x1=fixed(28), X=fixed(X_img_std), scaler=fixed(scaler_img))\n",
    "def show_explained_variance_reconst(image_idx, x0, x1, X, scaler):\n",
    "    \"\"\"\n",
    "    Shows the reconstruction of image `X[image_idx]` for different values `c` of explained\n",
    "    variances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_idx : int\n",
    "        Index of image to be reconstructed.\n",
    "    x0 : int\n",
    "        Number of pixels in first dimension.\n",
    "    x1 : int\n",
    "        Number of pixels in second dimension.\n",
    "    X : array-like, shape (N, D)\n",
    "        Images to be reconstracted.\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler to invert standardization.\n",
    "    \"\"\"\n",
    "    # Define list of explained variances.\n",
    "    c_list = np.array([0.01, 0.10, 0.50, 0.90, 0.99])\n",
    "    \n",
    "    # Plot original digit.\n",
    "    fig, ax = plt.subplots(1, len(c_list)+1, figsize=(20, 4))\n",
    "    ax[0].imshow(scaler.inverse_transform([X[image_idx]]).reshape(x0, x1), cmap='gray')\n",
    "    ax[0].set_title('Original Image', fontsize=14)\n",
    "        \n",
    "    # Plot reconstructed digits for different values of explained variances.\n",
    "    for i, c in enumerate(c_list):\n",
    "        # Fit PrincipalComponentAnalysis with explained variance `c`.\n",
    "        # TODO\n",
    "        \n",
    "        # Transform instance with index `image_idx` and save it as `z_selected`.\n",
    "        # TODO\n",
    "        \n",
    "        # Reconstruct instance `x_rec_selected` from `z_selected`.\n",
    "        # TODO\n",
    "        \n",
    "        # Plot reconstructed instance with index image_idx.\n",
    "        ax[i+1].imshow(scaler.inverse_transform(x_rec_selected).reshape(x0, x1), cmap='gray')\n",
    "        ax[i+1].set_title(\n",
    "            f'Explained Variance: $c={c}$,\\n Number of Components $M={pca.n_components_}$', fontsize=14\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "CXC11",
   "launcher_item_id": "ub5A7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
