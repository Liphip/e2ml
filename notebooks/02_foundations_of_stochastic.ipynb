{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cb72d03",
   "metadata": {},
   "source": [
    "# Foundations of Stochastic\n",
    "\n",
    "In this notebook, we will intensify our knowledge about the foundations of stochastic. \n",
    "\n",
    "At the start, we will introduce and analyze the properties of a binomial distribution.\n",
    "Subsequently, we introduce and analyze normal distributions.\n",
    "Finally, we will work with probability rules.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Discrete Probabilities](#discrete-probabilities)\n",
    "2. [Continuous Probabilities](#continuous-probabilities)\n",
    "3. [Probability Rules](#probability-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a714745d-6c84-452f-b796-681088b60161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b2b1b5",
   "metadata": {},
   "source": [
    "### **1. Discrete Probabilities** <a class=\"anchor\" id=\"discrete-probabilities\"></a>\n",
    "\n",
    "A discrete random variable $X$ taking values in the set $X(\\Omega) = \\{0, \\dots, n\\}, n \\in \\mathbb{N}_{\\geq 0}$ is said to be binomially distributed $X \\sim \\mathrm{Bin}(n, p)$ with $n$ as the number of trials and $p \\in [0, 1]$ as the success probability, if the *probability mass function (PMF) or probability distribution* $P(X)$ can be denoted by \n",
    "\n",
    "_Answer:_ <br/>\n",
    "$P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, x \\in X(\\Omega)$, where $\\binom{n}{x} = \\frac{n!}{x! (n-x)!}$ is the binomial coefficient.\n",
    "\n",
    "#### **Questions:**\n",
    "1. (a) How can we prove that the PMF $P(X)$ of a binomially distributed variable $X \\sim \\mathrm{Bin}(n,p)$ is normalized? \n",
    "\n",
    "    *Remark: One can use the binomial expansion known from elementary algebra.* \n",
    "    \n",
    "    _Answer:_ <br/> \n",
    "    Concept of binomial expension: $\\forall x, y \\in \\mathbb{R}, \\forall z \\in \\mathbb{N}_{\\geq 0} (x+y)^z = \\sum_{k=0}^z \\binom{z}{k} * x^k * y^{z-k}$. Therefore $\\sum P(X=x) = \\sum \\binom{x}{k} * p^x * (1-p)^{n-x} = (p + {(1-p)})^n = 1^n = 1$\n",
    "\n",
    "    (b) Define the probability space $(\\Omega, \\mathcal{A}, P)$ and a random variable $X$ modeling the number of heads when tossing a coin five times.\n",
    "    \n",
    "    _Answer:_ <br/>\n",
    "        The probability space $(\\Omega, \\mathcal{A}, P)$ is defined by $\\Omega = \\{H, T\\}^5$, where $H = Heads$ and $T = Tails$. <br/>\n",
    "        The event space $\\mathcal{A}$ is definded by $\\mathcal{A} = 2^{\\Omega}$. <br/>\n",
    "        The probability function $P$ is defined by \n",
    "        $$\n",
    "            P(\\mathcal{A})=\n",
    "            \\begin{cases}\n",
    "            0 \\text{   if A = $\\empty$,}\\\\\n",
    "            0.5^{|H|} * (1 - 0.5)^{5-|H|} \\text{   if |A| = 1,}\\\\\n",
    "            \\sum_{\\omega \\in A} P(\\{\\omega\\}) \\text{   else.}\n",
    "            \\end{cases}\n",
    "            =\n",
    "            \\begin{cases}\n",
    "            0 \\text{   if A = $\\empty$,}\\\\\n",
    "            0.5^{|H|} * 0.5^{5-|H|} \\text{   if |A| = 1,}\\\\\n",
    "            \\sum_{\\omega \\in A} P(\\{\\omega\\}) \\text{   else.}\n",
    "            \\end{cases}\n",
    "            =\n",
    "            \\begin{cases}\n",
    "            0 \\text{   if A = $\\empty$,}\\\\\n",
    "            0.5^5 \\text{   if |A| = 1,}\\\\\n",
    "            \\sum_{\\omega \\in A} P(\\{\\omega\\}) \\text{   else.}\n",
    "            \\end{cases}\n",
    "            =\n",
    "            \\begin{cases}\n",
    "            0 \\text{   if A = $\\empty$,}\\\\\n",
    "            0.03125 \\text{   if |A| = 1,}\\\\\n",
    "            \\sum_{\\omega \\in A} P(\\{\\omega\\}) \\text{   else.}\n",
    "            \\end{cases}\n",
    "        $$ <br/>\n",
    "        The random variable $X$ is defined by $X((\\omega_1, ..., \\omega_N)^T) = \\sum_{n=1}^N \\delta(\\omega_n = H)$, where $\\delta$ is the indicator function. <br/>\n",
    "    \n",
    "    (c) How can we derive the expected value $E(X)$ and the variance $V(X)$ of a binomially distributed variable $X \\sim \\mathrm{Bin}(n,p)$? \n",
    "\n",
    "    *Remark: A binomially distributed random variable can be represented through a sum of independent random variables following the same Bernoulli distribution, i.e.,* \n",
    "    $$X = \\sum_{i=1}^n X_i \\text{ with } \\forall i \\in \\{1, \\dots, n\\}: X_i \\sim \\mathrm{Bern}(p).$$\n",
    "\n",
    "    _Answer:_ <br/>\n",
    "    $E(X) = E(\\sum_{i=1}^n X_i) = \\sum_{i=1}^n E(X_i) = \\sum_{i=1}^n p = np$, since $E(X_i) = p, \\forall i \\in \\{1, \\dots, n\\}$. <br/>\n",
    "    TODO Varian to be added in next exercise after next lecture.\n",
    "\n",
    "    (d) How can we estimate the expected value and variance of the Binomial distribution, when we have made the observations $x_1, \\dots, x_N \\in \\{0, \\dots, n\\}$, $N \\in \\mathbb{N}_{>0}$?\n",
    "\n",
    "    TODO To be added in next exercise after next lecture.\n",
    "\n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to plot the binomial distribution for different values of $n$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdc81669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eac452b31e4a85af264789dba89daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', min=1), FloatSlider(value=0.5, description='p', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_binomial_distribution(n, p, N):\n",
    "    \"\"\"\n",
    "    Visualizes the binomial distribution for varying parameters and\n",
    "    indicates summary statistics, i.e., (empirical) mean and (empirical variance).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Positive number of trials within one binomial experiment.\n",
    "    p : float in [0, 1]\n",
    "        Success probability.\n",
    "    N : int\n",
    "        Positive number of repeated binomial experiments.\n",
    "    \"\"\"\n",
    "    # Compute the expected value `mean` and the variance `var`.\n",
    "    # V(x) = ... = n * p * (1 - p) To be discussed in the lecture and next exercise.\n",
    "    mean = n * p\n",
    "    var = n * p * (1 - p)\n",
    "    \n",
    "    # Draw N observations `x_sampled` from the PMF P(X) with X ~ Bin(n, p).\n",
    "    # x_smapled = stats.binom.rvs(n, p, size=N)\n",
    "    X = stats.binom(n, p)\n",
    "    x_sampled = X.rvs(size=N)\n",
    "    \n",
    "    # Estimate the expected value `mean_est` and variance `var_est` using the observations.\n",
    "    # Empirical Mean: x' = 1/N * sum_n=1^N x_n  To be discussed in the lecture and next exercise.\n",
    "    # Empirical Covariance O'^2 = 1/N-1 * sum_n=1^N (x_n - X')^2  To be discussed in the lecture and next exercise.\n",
    "    mean_est = 1/N * x_sampled.sum()\n",
    "    var_est = 1/(N-1) * np.sum((x_sampled - mean_est)**2)\n",
    "\n",
    "    \n",
    "    # Create an array `x` containing the numbers {0, ..., n}.\n",
    "    x = np.arange(n+1)\n",
    "    \n",
    "    # Compute P(X=x) as `p_x` with x in {0, ..., n} for X ~ Bin(n, p)\n",
    "    p_x = X.pmf(x)\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.bar(x, p_x, label=f'PMF')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$P(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathrm{Bin}(\" \n",
    "        + str(n) + \",\" \n",
    "        + str(p) \n",
    "        + \")$ with \\n$E(X) =$\" \n",
    "        + str(np.round(mean, 2)) \n",
    "        + \", $\\overline{x} =$\" \n",
    "        + str(np.round(mean_est, 2)) \n",
    "        + \", \\n$V(X) = $\" \n",
    "        + str(np.round(var, 2))\n",
    "        + \", $\\overline{\\sigma}^2 =$\" \n",
    "        + str(np.round(var_est, 2)) \n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_binomial_distribution, \n",
    "    n=IntSlider(value=10, min=1, max=100),\n",
    "    p=FloatSlider(value=0.5, min=0.0, max=1.0),\n",
    "    N=IntSlider(value=10, min=2, max=1000)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1152ed4",
   "metadata": {},
   "source": [
    "### **Questions:**\n",
    "1. (c) How does the sample size $N$ affect the estimates of the empirical mean and variance?\n",
    "   \n",
    "    _Answer:_ <br/>\n",
    "    The larger the sample size $N$, the closer the empirical mean and variance are to the true mean and variance.\n",
    "\n",
    "\n",
    "### **2. Continuous Probabilities** <a class=\"anchor\" id=\"continuous-probabilities\"></a>\n",
    "\n",
    "A continuous random variable $X$ taking any value in the set $X(\\Omega) = \\mathbb{R}$ is said to be rectangularly (uniformly) distributed $X \\sim \\mathrm{Rect}(a, b)$ with $a, b \\in \\mathbb{R}, a < b$ as its parameters, if the *probability density function (PDF)* $f(X)$ can be denoted by \n",
    "\n",
    "_Answer:_ <br/>\n",
    "    $\n",
    "        f(x) =\n",
    "        \\begin{cases}\n",
    "        \\frac{1}{b-a} \\text{   if } x \\in [a, b],\\\\\n",
    "        0 \\text{   else.}\n",
    "        \\end{cases}\n",
    "    $\n",
    "\n",
    "#### **Questions:**\n",
    "2. (a) How can we show that the PDF of the rectangularly distributed random variable $X \\sim \\mathrm{Rect}(a,b)$ is a valid PDF?\n",
    "   \n",
    "    _Answer:_ <br/>\n",
    "        The PDF $f: \\mathbb{R}^D -> \\mathbb{R}$ of a random variable $X$ is a valid PDF if it satisfies the following conditions: <br/>\n",
    "            1. it's integral exists, <br/>\n",
    "            2. $f(x) \\geq 0$ for all $x \\in \\mathbb{R}^D$, <br/>\n",
    "            3. and $\\int_{x \\in \\mathbb{R}^D} f(x) dx = \\int_{-\\infty}^\\infty ... \\int_{-\\infty}^\\infty f((x_1, ..., x_D)^T) dx_1 ... dx_D = 1$. <br/>\n",
    "        The first condition is trivially satisfied, since the integral of a constant function is a constant. <br/>\n",
    "        The second condition is trivially satisfied, since $f(x) = 0$ for $x \\notin [a, b]$ and $f(x) = \\frac{1}{a-b}$ for $x \\int [a, b]$. <br/>\n",
    "        The third condition is trivially satisfied, since $\\int_{x \\in [a, b]} f(x) dx = \\frac{1}{b-a} \\cdot (b-a) = 1$. <br/>\n",
    "\n",
    "**Definition 2.17** <font color='red'>**Multivariate Normal Distribution**</font> \n",
    "\n",
    "A multivariate continuous random variable $\\mathbf{X} = (X_1, \\dots, X_D)^\\mathrm{T}, D \\in \\mathbb{N}_{>0}$ follows a *multivariate normal distribution* with the mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^D$ and the symmetric, positive-definite covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D \\times D}$\n",
    "if the PDF is defined through\n",
    "$$\n",
    "f(\\mathbf{X}=\\mathbf{x}) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}} \\cdot \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\cdot \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\mathrm{T} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right),\n",
    "$$\n",
    "where $|\\boldsymbol{\\Sigma}|$ denotes the determinant of the covariance matrix and $\\boldsymbol{\\Sigma}^{-1}$ its inverse.\n",
    "\n",
    "**Remarks:**\n",
    "- The normal distribution, also known as the Gaussian distribution, is one of the most important probability distributions in stochastic. It is used in a wide range of fields to model the distribution of random variables that arise in nature, such as the heights of people, the weights of objects, and the errors in measurements.\n",
    "- We denote $\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ to indicate that a random variable follows a multivariate normal distribution.\n",
    "- The inverse of the covariance matrix, i.e., $\\boldsymbol{\\Sigma}^{-1}$, is also named precision matrix.\n",
    "\n",
    "#### **Questions:**\n",
    "2. (b) Which form does the PDF of a univariate normal distribution ($D=1$) take?\n",
    "   \n",
    "    _Answer:_ <br/>\n",
    "    $f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\cdot \\exp\\left(-\\frac{(x - \\mu)^2}{2 * \\sigma^2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9992e9ef-a436-4dba-8ac3-5226b1ba37f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ace566cb4d84c29bacb24d1115ad672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=2.0, min=-2.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_normal_distribution(mu, sigma, N):\n",
    "    \"\"\"\n",
    "    Visualizes the univariate normal distribution for varying parameters and\n",
    "    indicates summary statistics, i.e., (empirical) mean and (empirical) variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : float\n",
    "        Mean of the normal distribution.\n",
    "    sigma : float\n",
    "        Standard deviation of the normal distribution.\n",
    "    N : int\n",
    "        Positive number of repeated binomial experiments.\n",
    "    \"\"\"\n",
    "    # Draw N observations `x_sampled` from the pdf f(X) with X ~ N(mu, sigma**2).\n",
    "    x_sampled = stats.norm(mu, sigma).rvs(size=N)\n",
    "\n",
    "    # Estimate the expected value `mean_est` and variance `var_est` using the observations.\n",
    "    mean_est = 1/N * x_sampled.sum()\n",
    "    var_est = 1/(N-1) * np.sum((x_sampled - mean_est)**2)\n",
    "    \n",
    "    # Create an array `x` of 1000 linearly distributed values in the range [-5, 5].\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Compute the density f(X=x) as `f_x` for all values in `x`.\n",
    "    f_x = stats.norm(mu, sigma).pdf(x)\n",
    "\n",
    "    # Plot the f(x) for mu and sigma over all values in `x`.\n",
    "    plt.plot(x, f_x, label=f'PDF')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathcal{N}(\" \n",
    "        + str(mu) + \",\" \n",
    "        + str(np.round(sigma**2, 2)) \n",
    "        + \")$ with \\n$\\overline{x} =$\" \n",
    "        + str(np.round(mean_est, 2)) \n",
    "        + \", \\n$\\overline{\\sigma}^2 =$\" \n",
    "        + str(np.round(var_est, 2)) \n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive(\n",
    "    visualize_normal_distribution, \n",
    "    mu=FloatSlider(value=0, min=-2, max=2, description='μ'),\n",
    "    sigma=FloatSlider(value=1, min=0.1, max=2, description='𝜎²'),\n",
    "    N=IntSlider(value=10, min=2, max=1000, description='N')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3224cab8",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (c) How do the parameters $\\mu$ and $\\sigma^2$ affect the shape of the PDF of the normal distribution?\n",
    "\n",
    "   _Answer:_ <br/>\n",
    "   The parameter $\\mu$ shifts the PDF along the x-axis, while the parameter $\\sigma^2$ controls the width, and thus the height, of the PDF. <br/>\n",
    "\n",
    "Even for the univariate case $D=1$, the CDF of the normal distribution cannot be expressed in terms of elementary functions. However, many numerical approximations are known. A typical approach is to transform any univariate normal distribution into a standard normal distribution.\n",
    "\n",
    "**Definition 2.18** <font color='red'>**Standard Normal Distribution**</font> \n",
    "\n",
    "The univariate normal distribution $\\mathcal{N}(0, 1)$ is called *standard normal distribution* and its CDF is denoted as $\\Phi: \\mathbb{R} \\rightarrow [0, 1]$.\n",
    "\n",
    "For computing the probabilities of a random variable $X \\sim \\mathcal{N}(0, 1)$, there are [lookup tables](https://en.wikipedia.org/wiki/Standard_normal_table) and any random variable following a univariate normal distribution can be transformed to follow the standard normal distribution.\n",
    "\n",
    "**Theorem 2.10** <font color='red'>**Transformation to a Standard Normal Distribution**</font> \n",
    "\n",
    "Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ be a random variable following the univariate normal distribution $\\mathcal{N}(0, 1)$. Then, we get:\n",
    "$$\n",
    "F(X=x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "**Remark**: The standard normal distribution is symmetric such that $\\forall x \\in \\mathbb{R}: \\Phi(-x) = 1 - \\Phi(x)$. \n",
    "\n",
    "#### **Questions:**\n",
    "\n",
    "2. (d) Does it hold that for a random variable $X$ with $E(X)=\\mu$ and $V(X)=\\sigma^2$, we get $E(Z)=0$ and $V(Z)=1$ for $Z=\\frac{(X-\\mu)}{\\sigma}$? Prove your answer.\n",
    "\n",
    "   _Answer:_ <br/>\n",
    "   $E(Z) = E(\\frac{(X-\\mu)}{\\sigma}) = E(\\frac{1}{\\sigma} \\cdot X - \\frac{\\mu}{\\sigma}) = \\frac{1}{\\sigma} \\cdot E(X) - \\frac{\\mu}{\\sigma} = \\frac{1}{\\sigma} \\cdot \\mu - \\frac{\\mu}{\\sigma} = \\frac{\\mu}{\\sigma} - \\frac{\\mu}{\\sigma} = 0$ <br/>\n",
    "   <br/>\n",
    "   $V(Z) = V(\\frac{(X-\\mu)}{\\sigma}) = V(\\frac{1}{\\sigma} \\cdot X - \\frac{\\mu}{\\sigma}) = \\frac{1}{\\sigma^2} \\cdot V(X) = \\frac{1}{\\sigma^2} \\cdot \\sigma^2 = 1$ <br/>\n",
    "   \n",
    "   \n",
    "   (e) What is the probability of $1 \\leq X < 6$ for $X \\sim \\mathcal{N}(2, 4)$? Answer this question by using the standard normal distribution with a [lookup table](https://en.wikipedia.org/wiki/Standard_normal_table).\n",
    "   \n",
    "   _Answer:_ <br/>\n",
    "   $F(X)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$ <br/>\n",
    "   $P(1 \\leq X < 6) = F(X=6) - F(X=1) = \\Phi\\left(\\frac{6-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{1-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{6-2}{2}\\right) - \\Phi\\left(\\frac{1-2}{2}\\right) = \\Phi(2) - \\Phi(-0.5) = \\Phi(2) - (1 - \\Phi(0.5)) = \\Phi(2) - 1 + \\Phi(0.5) = 0.97725 - 1 + 0.69146 = -0.02275 + 0.69146 = 0.66871$ <br/>\n",
    "\n",
    "   \n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to verify the result in question 2(d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9131300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(1 <= X < 6) = 0.9772498680518208 - 0.3085375387259869 = 0.6687123293258339\n"
     ]
    }
   ],
   "source": [
    "# Compute the probability P(1 <= X < 6) for X ~ N(2, 4).\n",
    "p_6 = stats.norm(2, np.sqrt(4)).cdf(6)\n",
    "p_1 = stats.norm(2, np.sqrt(4)).cdf(1)\n",
    "p = p_6 - p_1\n",
    "print(f\"P(1 <= X < 6) = {p_6} - {p_1} = {p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca27383a",
   "metadata": {},
   "source": [
    "One of the main reasons the normal distribution is so widely used is due to the central limit theorem.\n",
    "\n",
    "**Theorem 2.11** <font color='red'>**Central Limit Theorem**</font> \n",
    "\n",
    "Let $X_1, X_2, \\dots $ be a sequence of i.i.d. random variables. Further, assume that the expected value $E(X_1) = \\mu$ and the variance $\\sigma^2 = V(X_1)$ exist. Then, the random variable $S_N = X_1 + \\dots + X_N, N \\in \\mathbb{N}_{>0}$ has an expected value of $E(S_N) = N\\mu$ and a variance of $V(S_N) = N\\sigma^2$. If one forms from it the standardized random variable\n",
    "$$\n",
    "Z_N = \\frac{S_N - N\\mu}{\\sigma\\sqrt{N}},\n",
    "$$\n",
    "then the central limit theorem states that the CDF of $Z_N$ for $N \\rightarrow \\infty$ pointwisely converges to the CDF $\\Phi$ of the standard normal distribution $\\mathcal{N}(0,1)$:\n",
    "$$\n",
    "\\lim_{N \\rightarrow \\infty} F(Z_n = z) = \\Phi(z).\n",
    "$$\n",
    "\n",
    "**Remarks:** \n",
    "- Intuitively, the theorem states that, under certain conditions, the sum of numerous i.i.d. random variables tends towards a normal distribution. This makes the normal distribution a natural choice for modeling a wide range of phenomena that arise in nature.\n",
    "- The earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivre–Laplace theorem.\n",
    "\n",
    "**Theorem 2.12** <font color='red'>**De Moivre-Laplace Theorem**</font> \n",
    "\n",
    "Let $X \\sim \\mathrm{Bin}(n, p)$ be a random variable with the expected value $E(X)=\\mu$ and the variance $V(X) = \\sigma$. Then, for a sufficiently large $n$ we can make the following approximation:\n",
    "$$\n",
    "F(X=x) = P(X \\leq x) \\approx \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "**Remark**: The following condition can serve as a rule of thumb for the application of the de Moivre-Laplace theorem\n",
    "$$\n",
    "V(X) = n \\cdot p \\cdot (1-p) > 9.\n",
    "$$\n",
    "\n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to compare the actual CDF of the binomial distribution with the one approximated via the de Moive-Laplace theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6affa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511370f546af4c69bf9b2a25d475b3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', min=1), FloatSlider(value=0.5, description='p', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_de_moive_laplace_theorem(n, p):\n",
    "    \"\"\"\n",
    "    Compares the CDF of the binomial distribution with the\n",
    "    approximation using the de Moivre-Laplace theorem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Positive number of trials within one binomial experiment.\n",
    "    p : float in [0, 1]\n",
    "        Success probability.\n",
    "    \"\"\"\n",
    "    # Compute the expected value `mean` and the variance `var`.\n",
    "    mean = n * p\n",
    "    var = n * p * (1 - p)\n",
    "    \n",
    "    # Create `x_bin` array containing the numbers {0, ..., n}.\n",
    "    x_bin = np.arange(n+1)\n",
    "     \n",
    "    # Compute F(X=x) as `f_x_bin` with x in {0, ..., n} for X ~ Bin(n, p)\n",
    "    f_x_bin = stats.binom(n, p).cdf(x_bin)\n",
    "    \n",
    "    # Create an array `x_norm` containing 10*n values evenly distributed across the interval [0, n].\n",
    "    x_norm = np.linspace(0, n, 10*n)\n",
    "    \n",
    "    # Use the de Moivre-Laplace theorem to approximate `f_x_bin` via `f_x_norm`.\n",
    "    f_x_norm = stats.norm(mean, np.sqrt(var)).cdf(x_norm)\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.bar(x_bin, f_x_bin, label=f'CDF of binomial distribution', color=\"blue\", alpha=0.5)\n",
    "    plt.plot(x_norm, f_x_norm, label=f'CDF of normal distribution', color=\"red\")\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$F(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathrm{Bin}(\" \n",
    "        + str(n) + \",\" \n",
    "        + str(p) \n",
    "        + \")$ with \\n$E(X) =$\" \n",
    "        + str(np.round(mean, 2)) \n",
    "        + \", \\n$V(X) = $\" \n",
    "        + str(np.round(var, 2))\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_de_moive_laplace_theorem, \n",
    "    n=IntSlider(value=10, min=1, max=100, description='n'),\n",
    "    p=FloatSlider(value=0.5, min=0.0, max=1.0, description='p'),\n",
    "    #N_=IntSlider(value=10, min=2, max=1000, description='N')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4636826",
   "metadata": {},
   "source": [
    "For the multivariate normal distribution, the covariance matrix $\\boldsymbol{\\sigma}$ plays a critical role regarding the distribution's shape. For a better understanding, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to study the influence of this matrix in the bivariate ($D=2$) case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2deedd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7932a107044afb893e42ded72ea9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mean_x', max=4.0, min=-4.0), FloatSlider(value=0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_bivariate_normal_distribution(mean_x, mean_y, var_x, var_y, cov_xy, N):\n",
    "    \"\"\"\n",
    "    Visualizes the bivariate normal distribution for varying parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean_x : float\n",
    "        Mean of the normal distribution in the x-dimension.\n",
    "    mean_y : float\n",
    "        Mean of the normal distribution in the y-dimension\n",
    "    var_x : float\n",
    "        Variance of the normal distribution in the x-dimension.\n",
    "    var_y : float\n",
    "        Variance of the normal distribution in the y-dimension.\n",
    "    cov_xy : float\n",
    "        Covariance of the normal distribution between the x- and y-dimension.\n",
    "    N : int\n",
    "        Positive number of observations to be drawn from the normal distribution.\n",
    "    \"\"\"\n",
    "    # Create the `mu` vector as numpy.ndarray.\n",
    "    mu = np.array([mean_x, mean_y])\n",
    "    \n",
    "    # Create the `Sigma` matrix as numpy.ndarray.\n",
    "    Sigma = np.array([\n",
    "        [var_x, cov_xy],\n",
    "        [cov_xy, var_y]\n",
    "    ])\n",
    "    \n",
    "    # Print error message if `Sigma` is not positiv definite.\n",
    "    if not np.all(np.linalg.eigvals(Sigma) > 0):\n",
    "        print(\"Sigma is not positive definite!\")\n",
    "        return\n",
    "    \n",
    "    # Draw N observations `X_sampled` from the PDF f(X) with X ~ Norm(mu, Sigma).\n",
    "    X_sampled = stats.multivariate_normal(mu, Sigma).rvs(N)\n",
    "    \n",
    "    # Plot sampled observations.\n",
    "    plt.scatter(X_sampled[:, 0], X_sampled[:, 1], label=\"sampled observations\")\n",
    "    plt.xlim([-10, 10])\n",
    "    plt.ylim([-10, 10])\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})$\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive(\n",
    "    visualize_bivariate_normal_distribution, \n",
    "    mean_x=FloatSlider(value=0, min=-4, max=4),\n",
    "    mean_y=FloatSlider(value=0, min=-4, max=4),\n",
    "    var_x=FloatSlider(value=1, min=0.1, max=4),\n",
    "    var_y=FloatSlider(value=1, min=0.1, max=4),\n",
    "    cov_xy=FloatSlider(value=0, min=-4, max=4),\n",
    "    N=IntSlider(value=1000, min=2, max=10000)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6981d10",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (f) How do the elements of the covariance matrix $\\boldsymbol{\\Sigma}$ affect the shape of the PDF of the normal distribution?\n",
    "\n",
    "   _Answer:_ <br/>\n",
    "   The covariance matrix $\\boldsymbol{\\Sigma}$ affects the shape of the PDF of the normal distribution by scaling the PDF in the direction of the eigenvectors of $\\boldsymbol{\\Sigma}$ with the eigenvalues as scaling factors. <br/>\n",
    "   The eigenvectors of $\\boldsymbol{\\Sigma}$ are the directions of the axes of the ellipsoid. The eigenvalues of $\\boldsymbol{\\Sigma}$ are the lengths of the axes of the ellipsoid. <br/>\n",
    "   \n",
    "### **3. Probability Rules** <a class=\"anchor\" id=\"probability-rules\"></a>\n",
    "Consider the following bivariate PMF $P(X, Y)$ of two discrete random variables $X$ and $Y$ with $X(\\Omega) = \\{x_1, x_2, x_3, x_4\\}$ and $Y(\\Omega) = \\{y_1, y_2, y_3\\}$:\n",
    "\n",
    "| $P(X=x_i, Y=y_i)$      | $x_1$ | $x_2$ | $x_3$ | $x_4$ |\n",
    "|------------------------|-------|-------|-------|-------|\n",
    "| $y_1$                  | 0.01  | 0.2   | 0.1   | 0.1   |\n",
    "| $y_2$                  | 0.05  | 0.05  | 0.07  | 0.2   |\n",
    "| $y_3$                  | 0.1   | 0.03  | 0.05  | 0.04  |\n",
    "\n",
    "#### **Questions:**\n",
    "3. (a) How can we compute the marginal PMFs $P(X)$ and $P(Y)$?\n",
    "    \n",
    "    _Answer:_ <br/>\n",
    "      $P(X=x_1) = P(X=x_1, Y=y_1) + P(X=x_1, Y=y_2) + P(X=x_1, Y=y_3) = 0.01 + 0.05 + 0.1 = 0.16$ <br/>\n",
    "      $P(X=x_2) = P(X=x_2, Y=y_1) + P(X=x_2, Y=y_2) + P(X=x_2, Y=y_3) = 0.2 + 0.05 + 0.03 = 0.28$ <br/>\n",
    "      $P(X=x_3) = P(X=x_3, Y=y_1) + P(X=x_3, Y=y_2) + P(X=x_3, Y=y_3) = 0.1 + 0.07 + 0.05 = 0.22$ <br/>\n",
    "      $P(X=x_4) = P(X=x_4, Y=y_1) + P(X=x_4, Y=y_2) + P(X=x_4, Y=y_3) = 0.1 + 0.2 + 0.04 = 0.34$ <br/>\n",
    "      $P(Y=y_1) = P(X=x_1, Y=y_1) + P(X=x_2, Y=y_1) + P(X=x_3, Y=y_1) + P(X=x_4, Y=y_1) = 0.01 + 0.2 + 0.1 + 0.1 = 0.41$ <br/>\n",
    "      $P(Y=y_2) = P(X=x_1, Y=y_2) + P(X=x_2, Y=y_2) + P(X=x_3, Y=y_2) + P(X=x_4, Y=y_2) = 0.05 + 0.05 + 0.07 + 0.2 = 0.37$ <br/>\n",
    "      $P(Y=y_3) = P(X=x_1, Y=y_3) + P(X=x_2, Y=y_3) + P(X=x_3, Y=y_3) + P(X=x_4, Y=y_3) = 0.1 + 0.03 + 0.05 + 0.04 = 0.22$ <br/>\n",
    "    \n",
    "   (b) How can we compute the conditional PMFs $P(X \\mid Y=y_1)$ and $P(Y \\mid X=x_3)$?\n",
    "    \n",
    "      _Answer:_ <br/>\n",
    "         $P(X \\mid Y=y_1) = \\frac{P(X=x_1, Y=y_1)}{P(Y=y_1)} = \\frac{0.01}{0.41} = 0.0244$ <br/>\n",
    "         <br/>\n",
    "         $P(Y \\mid X=x_3) = \\frac{P(X=x_3, Y=y_1)}{P(X=x_3)} = \\frac{0.1}{0.22} = 0.4545$ <br/>\n",
    "\n",
    "    \n",
    "     (b) Are the random variables $X$ and $Y$ statistically independent?\n",
    "    \n",
    "      _Answer:_ <br/>\n",
    "         $P(X=x_1, Y=y_1) = 0.01 \\neq 0.16 \\cdot 0.41 = P(X=x_1) \\cdot P(Y=y_1)$ <br/>\n",
    "         $P(X=x_1, Y=y_2) = 0.05 \\neq 0.16 \\cdot 0.37 = P(X=x_1) \\cdot P(Y=y_2)$ <br/>\n",
    "         $P(X=x_1, Y=y_3) = 0.1 \\neq 0.16 \\cdot 0.22 = P(X=x_1) \\cdot P(Y=y_3)$ <br/>\n",
    "         $P(X=x_2, Y=y_1) = 0.2 \\neq 0.28 \\cdot 0.41 = P(X=x_2) \\cdot P(Y=y_1)$ <br/>\n",
    "         $P(X=x_2, Y=y_2) = 0.05 \\neq 0.28 \\cdot 0.37 = P(X=x_2) \\cdot P(Y=y_2)$ <br/>\n",
    "         $P(X=x_2, Y=y_3) = 0.03 \\neq 0.28 \\cdot 0.22 = P(X=x_2) \\cdot P(Y=y_3)$ <br/>\n",
    "         $P(X=x_3, Y=y_1) = 0.1 \\neq 0.22 \\cdot 0.41 = P(X=x_3) \\cdot P(Y=y_1)$ <br/>\n",
    "         $P(X=x_3, Y=y_2) = 0.07 \\neq 0.22 \\cdot 0.37 = P(X=x_3) \\cdot P(Y=y_2)$ <br/>\n",
    "         $P(X=x_3, Y=y_3) = 0.05 \\neq 0.22 \\cdot 0.22 = P(X=x_3) \\cdot P(Y=y_3)$ <br/>\n",
    "         $P(X=x_4, Y=y_1) = 0.1 \\neq 0.34 \\cdot 0.41 = P(X=x_4) \\cdot P(Y=y_1)$ <br/>\n",
    "         $P(X=x_4, Y=y_2) = 0.2 \\neq 0.34 \\cdot 0.37 = P(X=x_4) \\cdot P(Y=y_2)$ <br/>\n",
    "         $P(X=x_4, Y=y_3) = 0.04 \\neq 0.34 \\cdot 0.22 = P(X=x_4) \\cdot P(Y=y_3)$ <br/>\n",
    "         Therefore, $X$ and $Y$ are not statistically independent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
