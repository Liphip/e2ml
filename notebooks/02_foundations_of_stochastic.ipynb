{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb72d03",
   "metadata": {},
   "source": [
    "# Foundations of Stochastic\n",
    "\n",
    "In this notebook, we will intensify our knowledge about the foundations of stochastic. \n",
    "\n",
    "At the start, we will introduce and analyze the properties of a binomial distribution.\n",
    "Subsequently, we introduce and analyze normal distributions.\n",
    "Finally, we will work with probability rules.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Discrete Probabilities](#discrete-probabilities)\n",
    "2. [Continuous Probabilities](#continuous-probabilities)\n",
    "3. [Probability Rules](#probability-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714745d-6c84-452f-b796-681088b60161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249219c",
   "metadata": {},
   "source": [
    "### **1. Discrete Probabilities** <a class=\"anchor\" id=\"discrete-probabilities\"></a>\n",
    "\n",
    "A discrete random variable $X$ taking values in the set $X(\\Omega) = \\{0, \\dots, n\\}, n \\in \\mathbb{N}_{\\geq 0}$ is said to be binomially distributed $X \\sim \\mathrm{Bin}(n, p)$ with $n$ as the number of trials and $p \\in [0, 1]$ as the success probability, if the *probability mass function (PMF) or probability distribution* $P(X)$ can be denoted by \n",
    "\n",
    "BEGIN SOLUTION\n",
    "$$\n",
    "P(X=x) = \\binom{n}{x} p^x(1-p)^{(n-x)}, x \\in X(\\Omega).\n",
    "$$\n",
    "END SOLUTION\n",
    "\n",
    "#### **Questions:**\n",
    "1. (a) How can we prove that the PMF $P(X)$ of a binomially distributed variable $X \\sim \\mathrm{Bin}(n,p)$ is normalized? \n",
    "\n",
    "    *Remark: One can use the binomial expansion known from elementary algebra.* \n",
    "    \n",
    "    BEGIN SOLUTION\n",
    "\n",
    "    According to the binomial expansion, we get\n",
    "    $$\n",
    "    \\forall x, y \\in \\mathbb{R}, \\forall z \\in \\mathbb{N}_{\\geq 0}: (x+y)^z = \\sum_{k=0}^{z} \\binom{z}{k} x^k y^{z-k}.\n",
    "    $$ \n",
    "    Defining $x=p$, $y=1-p$, and $z=n$, we can show the normalization:\n",
    "    $$\n",
    "    \\sum_{k=0}^{n} P(X=k) = \\sum_{k=0}^{n} \\binom{n}{k} p^k(1-p)^{n-k} = (p + (1-p))^n = 1^n = 1.\n",
    "    $$ \n",
    "\n",
    "    END SOLUTION\n",
    "    \n",
    "    (b) Define the probability space $(\\Omega, \\mathcal{A}, P)$ and a random variable $X$ modeling the number of heads when tossing a coin five times.\n",
    "    \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    We define the sample space $\\Omega = \\{H, T\\}^5$ with $H$ as head and $T$ as tail, the event space $\\mathcal{A} = 2^\\Omega$ as the power set of the sample space, and the probability measure through\n",
    "    $$\n",
    "    P(A) = \\begin{cases} 0, \\text{ if } A = \\emptyset, \\\\ 0.5^5 \\text{ if } |A| = 1 \\\\ \\sum_{\\omega \\in A} P(\\{\\omega\\}) \\text{ else.} \\end{cases}\n",
    "    $$\n",
    "    The random variable is binomially distributed, i.e., $X \\sim \\mathrm{Bin}(n=5, p=0.5)$ with \n",
    "    $$\n",
    "    X(\\omega_1, \\dots, \\omega_5) = \\sum_{i=1}^5 \\delta(\\omega_i = H), \n",
    "    $$\n",
    "    where $\\delta$ is an indicator function.\n",
    "    \n",
    "    END SOLUTION\n",
    "    \n",
    "    (c) How can we derive the expected value $E(X)$ and the variance $V(X)$ of a binomially distributed variable $X \\sim \\mathrm{Bin}(n,p)$? \n",
    "\n",
    "    *Remark: A binomially distributed random variable can be represented through a sum of independent random variables following the same Bernoulli distribution, i.e.,* \n",
    "    $$X = \\sum_{i=1}^n X_i \\text{ with } \\forall i \\in \\{1, \\dots, n\\}: X_i \\sim \\mathrm{Bern}(p).$$\n",
    "\n",
    "    BEGIN SOLUTION\n",
    "\n",
    "    According to Theorem 2.4 (Linearity of Expected Values), we can compute the expected value as:\n",
    "    $$E(X) = E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E\\left(X_i\\right) = \\sum_{i=1}^n p = np,$$\n",
    "    where we use $E(X_i) = p$ as the expected value of a Bernoulli distributed random variable.\n",
    "\n",
    "    According to Theorem 2.9 (Properties of Statistical Variance), we know the variances of statistically independent random variables are additive. Hence, we can compute the variance as:\n",
    "    $$V(X) = V\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n V\\left(X_i\\right) = \\sum_{i=1}^n p(1-p) = np(1-p),$$\n",
    "    where we use $V(X_i) = p(1-p)$ as the variance of a Bernoulli distributed random variable.\n",
    "\n",
    "    END SOLUTION  \n",
    "\n",
    "    (d) How can we estimate the expected value and variance of the Binomial distribution, when we have made the observations $x_1, \\dots, x_N \\in \\{0, \\dots, n\\}$, $N \\in \\mathbb{N}_{>0}$?\n",
    "\n",
    "    BEGIN SOLUTION\n",
    "\n",
    "    We can use Definition 2.13 (Empirical Mean) to estimate the expected value as:\n",
    "    $$\\overline{x} = \\frac{1}{N} \\sum_{i=1}^N x_i$$\n",
    "    and the Definition 2.14 (Empirical Covariance) to estimate the variance as:\n",
    "    $$\\overline{\\sigma}^2 = \\frac{1}{N-1} \\sum_{i=1}^N (\\overline{x}-x_i)^2.$$\n",
    "\n",
    "    END SOLUTION\n",
    "\n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to plot the binomial distribution for different values of $n$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc81669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_binomial_distribution(n, p, N):\n",
    "    \"\"\"\n",
    "    Visualizes the binomial distribution for varying parameters and\n",
    "    indicates summary statistics, i.e., (empirical) mean and (empirical variance).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Positive number of trials within one binomial experiment.\n",
    "    p : float in [0, 1]\n",
    "        Success probability.\n",
    "    N : int\n",
    "        Positive number of repeated binomial experiments.\n",
    "    \"\"\"\n",
    "    # Compute the expected value `mean` and the variance `var`.\n",
    "    # BEGIN SOLUTION\n",
    "    mean = n * p\n",
    "    var = n * p * (1-p)\n",
    "    # END SOLUTION\n",
    "    \n",
    "    # Draw N observations `x_sampled` from the PMF P(X) with X ~ Bin(n, p).\n",
    "    x_sampled = stats.binom(n, p).rvs(N) # <-- SOLUTION\n",
    "    \n",
    "    # Estimate the expected value `mean_est` and variance `var_est` using the observations.\n",
    "    # BEGIN SOLUTION\n",
    "    mean_est = x_sampled.sum() / N\n",
    "    var_est = ((x_sampled - mean_est)**2).sum() / (N-1)\n",
    "    # END SOLUTION\n",
    "    \n",
    "    # Create an array `x` containing the numbers {0, ..., n}.\n",
    "    x = np.arange(0, n+1) # <-- SOLUTION\n",
    "    \n",
    "    # Compute P(X=x) as `p_x` with x in {0, ..., n} for X ~ Bin(n, p)\n",
    "    p_x = stats.binom(n, p).pmf(x) # <-- SOLUTION\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.bar(x, p_x, label=f'PMF')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$P(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathrm{Bin}(\" \n",
    "        + str(n) + \",\" \n",
    "        + str(p) \n",
    "        + \")$ with \\n$E(X) =$\" \n",
    "        + str(np.round(mean, 2)) \n",
    "        + \", $\\overline{x} =$\" \n",
    "        + str(np.round(mean_est, 2)) \n",
    "        + \", \\n$V(X) = $\" \n",
    "        + str(np.round(var, 2))\n",
    "        + \", $\\overline{\\sigma}^2 =$\" \n",
    "        + str(np.round(var_est, 2)) \n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_binomial_distribution, \n",
    "    n=IntSlider(value=10, min=1, max=100),\n",
    "    p=FloatSlider(value=0.5, min=0.0, max=1.0),\n",
    "    N=IntSlider(value=10, min=2, max=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1152ed4",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "1. (c) How does the sample size $N$ affect the estimates of the empirical mean and variance?\n",
    "   \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    Increasing the sample size $N$ leads to more accurate empirical estimates, i.e., they get closer to the true\n",
    "    statistics.\n",
    "    \n",
    "    END SOLUTION  \n",
    "\n",
    "\n",
    "### **2. Continuous Probabilities** <a class=\"anchor\" id=\"continuous-probabilities\"></a>\n",
    "\n",
    "A continuous random variable $X$ taking any value in the set $X(\\Omega) = \\mathbb{R}$ is said to be rectangularly (uniformly) distributed $X \\sim \\mathrm{Rect}(a, b)$ with $a, b \\in \\mathbb{R}, a < b$ as its parameters, if the *probability density function (PDF)* $f(X)$ can be denoted by \n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "$$\n",
    "f(X=x) = \\begin{cases} \\frac{1}{b-a}, \\text{ if } x \\in [a, b] \\\\ 0, \\text{ else.} \\end{cases}\n",
    "$$\n",
    "\n",
    "END SOLUTION\n",
    "\n",
    "#### **Questions:**\n",
    "2. (a) How can we show that the PDF of the rectangularly distributed random variable $X \\sim \\mathrm{Rect}(a,b)$ is a valid PDF?\n",
    "   \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    We need to show the properties (1-3) given in Definition 2.8.\n",
    "    \n",
    "    (1) The integral of $f$ exists, since $f$ is continuous in $\\mathbb{R}$ except for the two jump points at $X=a$ and $X=b$.\n",
    "    \n",
    "    (2) The density is always non-negative, since $f(X=x) = 1 \\geq 0\\, \\forall x \\in [a, b]$ and $f(X=x) = 0 \\geq 0\\, \\forall x \\in (-\\infty, a) \\cup (b, \\infty)$.\n",
    "    \n",
    "    (3) The function $f$ is normalized according to\n",
    "    $$\n",
    "    \\int_{-\\infty}^{\\infty} f(X=x)\\mathrm{d}x = \\int_{a}^{b} \\frac{1}{b-a}\\mathrm{d}x = \\left[\\frac{x}{b-a}\\right]^b_a = \\frac{b-a}{b-a} = 1.\n",
    "    $$\n",
    "    \n",
    "    END SOLUTION \n",
    "\n",
    "**Definition 2.17** <font color='red'>**Multivariate Normal Distribution**</font> \n",
    "\n",
    "A multivariate continuous random variable $\\mathbf{X} = (X_1, \\dots, X_D)^\\mathrm{T}, D \\in \\mathbb{N}_{>0}$ follows a *multivariate normal distribution* with the mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^D$ and the symmetric, positive-definite covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D \\times D}$\n",
    "if the PDF is defined through\n",
    "$$\n",
    "f(\\mathbf{X}=\\mathbf{x}) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}} \\cdot \\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\cdot \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\mathrm{T} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right),\n",
    "$$\n",
    "where $|\\boldsymbol{\\Sigma}|$ denotes the determinant of the covariance matrix and $\\boldsymbol{\\Sigma}^{-1}$ its inverse.\n",
    "\n",
    "**Remarks:**\n",
    "- The normal distribution, also known as the Gaussian distribution, is one of the most important probability distributions in stochastic. It is used in a wide range of fields to model the distribution of random variables that arise in nature, such as the heights of people, the weights of objects, and the errors in measurements.\n",
    "- We denote $\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ to indicate that a random variable follows a multivariate normal distribution.\n",
    "- The inverse of the covariance matrix, i.e., $\\boldsymbol{\\Sigma}^{-1}$, is also named precision matrix.\n",
    "\n",
    "#### **Questions:**\n",
    "2. (b) Which form does the PDF of a univariate normal distribution ($D=1$) take?\n",
    "   \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    For $D=1$, the covariance matrix $\\boldsymbol{\\Sigma}$ reduces to the variance $\\sigma^2$. Accordingly, we obtain:\n",
    "    $$\n",
    "    f(X=x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n",
    "    $$\n",
    "    \n",
    "    END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992e9ef-a436-4dba-8ac3-5226b1ba37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_normal_distribution(mu, sigma, N):\n",
    "    \"\"\"\n",
    "    Visualizes the univariate normal distribution for varying parameters and\n",
    "    indicates summary statistics, i.e., (empirical) mean and (empirical) variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : float\n",
    "        Mean of the normal distribution.\n",
    "    sigma : float\n",
    "        Standard deviation of the normal distribution.\n",
    "    N : int\n",
    "        Positive number of repeated binomial experiments.\n",
    "    \"\"\"\n",
    "    # Draw N observations `x_sampled` from the pdf f(X) with X ~ N(mu, sigma**2).\n",
    "    x_sampled = stats.norm(mu, sigma).rvs(N) # <-- SOLUTION\n",
    "    \n",
    "    # Estimate the expected value `mean_est` and variance `var_est` using the observations.\n",
    "    # BEGIN SOLUTION\n",
    "    mean_est = x_sampled.sum() / N\n",
    "    var_est = ((x_sampled - mean_est)**2).sum() / (N-1)\n",
    "    # END SOLUTION\n",
    "    \n",
    "    # Create an array `x` of 1000 linearly distributed values in the range [-5, 5].\n",
    "    x = np.linspace(-5, 5, 1000) # <-- SOLUTION\n",
    "    \n",
    "    # Compute the density f(X=x) as `f_x` for all values in `x`.\n",
    "    f_x = stats.norm(mu, sigma).pdf(x)\n",
    "\n",
    "    # Plot the f(x) for mu and sigma over all values in `x`.\n",
    "    plt.plot(x, f_x, label=f'PDF')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathcal{N}(\" \n",
    "        + str(mu) + \",\" \n",
    "        + str(np.round(sigma**2, 2)) \n",
    "        + \")$ with \\n$\\overline{x} =$\" \n",
    "        + str(np.round(mean_est, 2)) \n",
    "        + \", \\n$\\overline{\\sigma}^2 =$\" \n",
    "        + str(np.round(var_est, 2)) \n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # END SOLUTION\n",
    "\n",
    "interactive(\n",
    "    visualize_normal_distribution, \n",
    "    mu=FloatSlider(value=0, min=-2, max=2),\n",
    "    sigma=FloatSlider(value=1, min=0.1, max=2),\n",
    "    N=IntSlider(value=10, min=2, max=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224cab8",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (c) How do the parameters $\\mu$ and $\\sigma^2$ affect the shape of the PDF of the normal distribution?\n",
    "\n",
    "   BEGIN SOLUTION\n",
    "   \n",
    "   Changing the mean $\\mu$ shifts the mode ($x$-value with maximum density) of the PDF on the x-axis. For a higher variance $\\sigma^2$, the PDF will be broader and shorter. However, if the variance $\\sigma^2$ is small, the curve will be narrow and tall near to the mean $\\mu$.\n",
    "   \n",
    "   END SOLUTION\n",
    "\n",
    "Even for the univariate case $D=1$, the CDF of the normal distribution cannot be expressed in terms of elementary functions. However, many numerical approximations are known. A typical approach is to transform any univariate normal distribution into a standard normal distribution.\n",
    "\n",
    "**Definition 2.18** <font color='red'>**Standard Normal Distribution**</font> \n",
    "\n",
    "The univariate normal distribution $\\mathcal{N}(0, 1)$ is called *standard normal distribution* and its CDF is denoted as $\\Phi: \\mathbb{R} \\rightarrow [0, 1]$.\n",
    "\n",
    "For computing the probabilities of a random variable $X \\sim \\mathcal{N}(0, 1)$, there are [lookup tables](https://en.wikipedia.org/wiki/Standard_normal_table) and any random variable following a univariate normal distribution can be transformed to follow the standard normal distribution.\n",
    "\n",
    "**Theorem 2.10** <font color='red'>**Transformation to a Standard Normal Distribution**</font> \n",
    "\n",
    "Let $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ be a random variable following the univariate normal distribution $\\mathcal{N}(0, 1)$. Then, we get:\n",
    "$$\n",
    "F(X=x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "**Remark**: The standard normal distribution is symmetric such that $\\forall x \\in \\mathbb{R}: \\Phi(-x) = 1 - \\Phi(x)$. \n",
    "\n",
    "#### **Questions:**\n",
    "\n",
    "2. (d) Does it hold that for a random variable $X$ with $E(X)=\\mu$ and $V(X)=\\sigma^2$, we get $E(Z)=0$ and $V(Z)=1$ for $Z=\\frac{(X-\\mu)}{\\sigma}$? Prove your answer.\n",
    "\n",
    "   BEGIN SOLUTION\n",
    "   \n",
    "   As the following proof shows, the answer is *yes*.\n",
    "   \n",
    "   According to Theorem 2.4 (Linearity of Expected Values), we get\n",
    "   $$\n",
    "   E(Z) = E\\left(\\frac{X-\\mu}{\\sigma}\\right) = E\\left(\\frac{1}{\\sigma} \\cdot X - \\frac{\\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}E\\left(X\\right) - \\frac{\\mu}{\\sigma} = \\frac{\\mu}{\\sigma} - \\frac{\\mu}{\\sigma} = 0.\n",
    "   $$\n",
    "   \n",
    "   According to Theorem 2.5 (Properties of Variance), we get\n",
    "   $$\n",
    "   V(Z) = V\\left(\\frac{X-\\mu}{\\sigma}\\right) = V\\left(\\frac{1}{\\sigma} \\cdot X - \\frac{\\mu}{\\sigma}\\right) = \\frac{1}{\\sigma^2}V\\left(X\\right) = \\frac{\\sigma^2}{\\sigma^2} = 1.\n",
    "   $$\n",
    "   \n",
    "   END SOLUTION\n",
    "   \n",
    "   (e) What is the probability of $1 \\leq X < 6$ for $X \\sim \\mathcal{N}(2, 4)$? Answer this question by using the standard normal distribution with a [lookup table](https://en.wikipedia.org/wiki/Standard_normal_table).\n",
    "   \n",
    "   BEGIN SOLUTION\n",
    "   \n",
    "   According to Theorem 2.2 (Probabilities of Arbitrary Intervals) and Theorem 2.10 (Transformation to Standard Normal distribution), we can compute\n",
    "   $$\n",
    "   P(1 \\leq X < 6) = F(X=6) - F(X=1) = \\Phi\\left(\\frac{6-2}{2}\\right) - \\Phi\\left(\\frac{1-2}{2}\\right) \\\\ \n",
    "   = \\Phi\\left(2\\right) - \\Phi\\left(-0.5\\right) = \\Phi\\left(2\\right) - (1-\\Phi\\left(0.5\\right)) \\approx 0.97725 - (1 - 0.69146) = 0.66871.\n",
    "   $$\n",
    "   \n",
    "   END SOLUTION\n",
    "   \n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to verify the result in question 2(d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9131300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability P(1 <= X < 6) for X ~ N(2, 4).\n",
    "p_6 = stats.norm(2, np.sqrt(4)).cdf(6)\n",
    "p_1 = stats.norm(2, np.sqrt(4)).cdf(1)\n",
    "p = p_6 - p_1\n",
    "print(f\"P(1 <= X < 6) = {p_6} - {p_1} = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27383a",
   "metadata": {},
   "source": [
    "One of the main reasons the normal distribution is so widely used is due to the central limit theorem.\n",
    "\n",
    "**Theorem 2.11** <font color='red'>**Central Limit Theorem**</font> \n",
    "\n",
    "Let $X_1, X_2, \\dots $ be a sequence of i.i.d. random variables. Further, assume that the expected value $E(X_1) = \\mu$ and the variance $\\sigma^2 = V(X_1)$ exist. Then, the random variable $S_N = X_1 + \\dots + X_N, N \\in \\mathbb{N}_{>0}$ has an expected value of $E(S_N) = N\\mu$ and a variance of $V(S_N) = N\\sigma^2$. If one forms from it the standardized random variable\n",
    "$$\n",
    "Z_N = \\frac{S_N - N\\mu}{\\sigma\\sqrt{N}},\n",
    "$$\n",
    "then the central limit theorem states that the CDF of $Z_N$ for $N \\rightarrow \\infty$ pointwisely converges to the CDF $\\Phi$ of the standard normal distribution $\\mathcal{N}(0,1)$:\n",
    "$$\n",
    "\\lim_{N \\rightarrow \\infty} F(Z_n = z) = \\Phi(z).\n",
    "$$\n",
    "\n",
    "**Remarks:** \n",
    "- Intuitively, the theorem states that, under certain conditions, the sum of numerous i.i.d. random variables tends towards a normal distribution. This makes the normal distribution a natural choice for modeling a wide range of phenomena that arise in nature.\n",
    "- The earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivreâ€“Laplace theorem.\n",
    "\n",
    "**Theorem 2.12** <font color='red'>**De Moivre-Laplace Theorem**</font> \n",
    "\n",
    "Let $X \\sim \\mathrm{Bin}(n, p)$ be a random variable with the expected value $E(X)=\\mu$ and the variance $V(X) = \\sigma$. Then, for a sufficiently large $n$ we can make the following approximation:\n",
    "$$\n",
    "F(X=x) = P(X \\leq x) \\approx \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "**Remark**: The following condition can serve as a rule of thumb for the application of the de Moivre-Laplace theorem\n",
    "$$\n",
    "V(X) = n \\cdot p \\cdot (1-p) > 9.\n",
    "$$\n",
    "\n",
    "In the following, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to compare the actual CDF of the binomial distribution with the one approximated via the de Moive-Laplace theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6affa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_de_moive_laplace_theorem(n, p):\n",
    "    \"\"\"\n",
    "    Compares the CDF of the binomial distribution with the\n",
    "    approximation using the de Moivre-Laplace theorem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Positive number of trials within one binomial experiment.\n",
    "    p : float in [0, 1]\n",
    "        Success probability.\n",
    "    \"\"\"\n",
    "    # Compute the expected value `mean` and the variance `var`.\n",
    "    # BEGIN SOLUTION\n",
    "    mean = n * p\n",
    "    var = n*p*(1-p)\n",
    "    # END SOLUTION\n",
    "    \n",
    "    # Create `x_bin` array containing the numbers {0, ..., n}.\n",
    "    x_bin = np.arange(0, n+1) # <-- SOLUTION\n",
    "     \n",
    "    # Compute F(X=x) as `f_x_bin` with x in {0, ..., n} for X ~ Bin(n, p)\n",
    "    f_x_bin = stats.binom(n, p).cdf(x_bin) # <-- SOLUTION\n",
    "    \n",
    "    # Create an array `x_norm` containing 10*n values evenly distributed across the interval [0, n].\n",
    "    x_norm = np.linspace(0, n, 10*n) # <-- SOLUTION\n",
    "    \n",
    "    # Use the de Moivre-Laplace theorem to approximate `f_x_bin` via `f_x_norm`.\n",
    "    f_x_norm = stats.norm(0, 1).cdf((x_norm-mean)/np.sqrt(var)) # <-- SOLUTION\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.bar(x_bin, f_x_bin, label=f'CDF of binomial distribution', color=\"blue\", alpha=0.5)\n",
    "    plt.plot(x_norm, f_x_norm, label=f'CDF of normal distribution', color=\"red\")\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$F(X=x)$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathrm{Bin}(\" \n",
    "        + str(n) + \",\" \n",
    "        + str(p) \n",
    "        + \")$ with \\n$E(X) =$\" \n",
    "        + str(np.round(mean, 2)) \n",
    "        + \", \\n$V(X) = $\" \n",
    "        + str(np.round(var, 2))\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_de_moive_laplace_theorem, \n",
    "    n=IntSlider(value=10, min=1, max=100),\n",
    "    p=FloatSlider(value=0.5, min=0.0, max=1.0),\n",
    "    N=IntSlider(value=10, min=2, max=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4636826",
   "metadata": {},
   "source": [
    "For the multivariate normal distribution, the covariance matrix $\\boldsymbol{\\sigma}$ plays a critical role regarding the distribution's shape. For a better understanding, we use the [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) package to study the influence of this matrix in the bivariate ($D=2$) case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2deedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bivariate_normal_distribution(mean_x, mean_y, var_x, var_y, cov_xy, N):\n",
    "    \"\"\"\n",
    "    Visualizes the bivariate normal distribution for varying parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean_x : float\n",
    "        Mean of the normal distribution in the x-dimension.\n",
    "    mean_y : float\n",
    "        Mean of the normal distribution in the y-dimension\n",
    "    var_x : float\n",
    "        Variance of the normal distribution in the x-dimension.\n",
    "    var_y : float\n",
    "        Variance of the normal distribution in the y-dimension.\n",
    "    cov_xy : float\n",
    "        Covariance of the normal distribution between the x- and y-dimension.\n",
    "    N : int\n",
    "        Positive number of observations to be drawn from the normal distribution.\n",
    "    \"\"\"\n",
    "    # Create the `mu` vector as numpy.ndarray.\n",
    "    mu = np.array([mean_x, mean_y]) # <-- SOLUTION\n",
    "    \n",
    "    # Create the `Sigma` matrix as numpy.ndarray.\n",
    "    Sigma = np.array([[var_x, cov_xy], [cov_xy, var_y]]) # <-- SOLUTION\n",
    "    \n",
    "    # Print error message if `Sigma` is not positiv definite.\n",
    "    # BEGIN SOLUTION\n",
    "    if np.any(np.linalg.eigvals(Sigma) < 0):\n",
    "        print(\"Sigma is not positive definite.\")\n",
    "        return\n",
    "    # END SOLUTION\n",
    "    \n",
    "    # Draw N observations `X_sampled` from the PDF f(X) with X ~ Norm(mu, Sigma).\n",
    "    X_sampled = stats.multivariate_normal(mu, Sigma).rvs(N) # <-- SOLUTION\n",
    "    \n",
    "    # Plot sampled observations.\n",
    "    plt.scatter(X_sampled[:, 0], X_sampled[:, 1], label=\"sampled observations\")\n",
    "    plt.xlim([-10, 10])\n",
    "    plt.ylim([-10, 10])\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title(\n",
    "        \"$X \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})$\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive(\n",
    "    visualize_bivariate_normal_distribution, \n",
    "    mean_x=FloatSlider(value=0, min=-4, max=4),\n",
    "    mean_y=FloatSlider(value=0, min=-4, max=4),\n",
    "    var_x=FloatSlider(value=1, min=0.1, max=4),\n",
    "    var_y=FloatSlider(value=1, min=0.1, max=4),\n",
    "    cov_xy=FloatSlider(value=0, min=-4, max=4),\n",
    "    N=IntSlider(value=1000, min=2, max=10000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6981d10",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (f) How do the elements of the covariance matrix $\\boldsymbol{\\Sigma}$ affect the shape of the PDF of the normal distribution?\n",
    "\n",
    "   BEGIN SOLUTION\n",
    "   \n",
    "   The entries on the diagonal corresponding to the variances in the $x$- and $y$-dimension quantify the spread accross these dimensions. The entries off the diagonal corresponding to the covariance quantify the linear relationship between both dimensions.\n",
    "   \n",
    "   END SOLUTION\n",
    "   \n",
    "### **3. Probability Rules** <a class=\"anchor\" id=\"probability-rules\"></a>\n",
    "Consider the following bivariate PMF $P(X, Y)$ of two discrete random variables $X$ and $Y$ with $X(\\Omega) = \\{x_1, x_2, x_3, x_4\\}$ and $Y(\\Omega) = \\{y_1, y_2, y_3\\}$:\n",
    "\n",
    "| $P(X=x_i, Y=y_i)$      | $x_1$ | $x_2$ | $x_3$ | $x_4$ |\n",
    "|------------------------|-------|-------|-------|-------|\n",
    "| $y_1$                  | 0.01  | 0.2   | 0.1   | 0.1   |\n",
    "| $y_2$                  | 0.05  | 0.05  | 0.07  | 0.2   |\n",
    "| $y_3$                  | 0.1   | 0.03  | 0.05  | 0.04  |\n",
    "\n",
    "#### **Questions:**\n",
    "3. (a) How can we compute the marginal PMFs $P(X)$ and $P(Y)$?\n",
    "    \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    We can use the Theorem 2.6 (Sum Rule) to specify both marginal PMFs:\n",
    "    \n",
    "    |            | $x_1$ | $x_2$ | $x_3$ | $x_4$     |\n",
    "    |------------|-------|-------|-------|-----------|\n",
    "    | $P(X=x_i)$ | 0.16  | 0.28  | 0.22  | 0.34  |\n",
    "    \n",
    "    |            | $y_1$ | $y_2$ | $y_3$     |\n",
    "    |------------|-------|-------|-----------|\n",
    "    | $P(Y=y_i)$ | 0.41  | 0.37  | 0.22      |\n",
    "        \n",
    "    END SOLUTION\n",
    "    \n",
    "   (b) How can we compute the conditional PMFs $P(X \\mid Y=y_1)$ and $P(Y \\mid X=x_3)$?\n",
    "    \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    We can use the Theorem 2.8 (Bayes' Theorem) to specify both conditional PMFs (both tables contain approximated values):\n",
    "    \n",
    "    |                       | $x_1$ | $x_2$ | $x_3$ | $x_4$  |\n",
    "    |-----------------------|-------|-------|-------|--------|\n",
    "    | $P(X=x_i \\mid Y=y_1)$ | 0.03  | 0.49  | 0.24  | 0.24   |\n",
    "    \n",
    "    |                       | $y_1$ | $y_2$ | $y_3$ |\n",
    "    |-----------------------|-------|-------|-------|\n",
    "    | $P(Y=y_i \\mid X=x_3)$ | 0.45  | 0.32  | 0.23  |\n",
    "    \n",
    "    END SOLUTION\n",
    "    \n",
    "     (b) Are the random variables $X$ and $Y$ statistically independent?\n",
    "    \n",
    "    BEGIN SOLUTION\n",
    "    \n",
    "    According to Definition 2.15 (Statistical Independence), they are not independent because of \n",
    "    $$\n",
    "    P(X=x_1, Y=y_1) = 0.01 \\neq 0.0656 = P(X=x_1) \\cdot P(Y=y_1).\n",
    "    $$\n",
    "    \n",
    "    END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ml-env",
   "language": "python",
   "name": "e2ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
