{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c70fa63-b796-4b94-b670-0e98bb66a5a1",
   "metadata": {},
   "source": [
    "# Design of Experiments with and for Machine Learning\n",
    "\n",
    "In this notebook, we will implement **Bayesian optimization** techniques as adative sampling methodes for experimental design. \n",
    "\n",
    "At the start, we will implement a *Gaussian process* (GP) for regression problems.\n",
    "\n",
    "Subsequently, we will use GPs as surrogate probabilistic models to implement the acquisition functions\n",
    "- *probability improvement* (PI),\n",
    "- *expected improvement* (EI),\n",
    "- and *upper confidence bound* (UBC).\n",
    "\n",
    "Finally, we will employ the implemented acquisition functions on synthethic data sets to study their behaviors.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Gaussian Processes for Regression](#gpr)\n",
    "2. [Acquisition Functions](#af)\n",
    "3. [Bayesian Optimization](#bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbb5690-6d49-42df-bf4b-db120f76c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, Dropdown, Button\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from functools import partial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e495a7",
   "metadata": {},
   "source": [
    "### **1. Gaussian Processes for Regression** <a class=\"anchor\" id=\"gpr\"></a>\n",
    "\n",
    "**Kernel functions** are central components of Gaussian processes. A kernel function $k: \\mathcal{X} \\rightarrow \\mathbb{R}$ is typically chosen to quantify a kind of similarity between two arbitrary instances $\\mathbf{x}_n$ and $\\mathbf{x}_m$. This similary measurement corresponds to a dot product of two instances in a transformed Hilbert space:\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\boldsymbol{\\phi}(\\mathbf{x})^\\mathrm{T} \\boldsymbol{\\phi}(\\mathbf{x}^\\prime),\n",
    "$$\n",
    "where $\\boldsymbol{\\phi}: \\mathcal{X} \\rightarrow \\mathcal{H}$ is the feature map of the corresponding kernel function. Accordingly, we can use this property to prove whether a function defines a valid kernel. As a simple example, consider a kernel function given by\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = (\\mathbf{x}^\\mathrm{T}\\mathbf{x}^\\prime)^2.\n",
    "$$\n",
    "If we take the particular case of a two-dimensional instance space $\\mathcal{X} = \\mathbb{R}^2$ we can expand out the terms and thereby identify the following nonlinear feature mapping:\n",
    "\n",
    "_Answer:_ <br/>\n",
    "$$\n",
    "k(x, x^\\prime) = (X^\\mathrm{T}, x^\\prime)^2 = (x_1 x_1^\\prime + x_2 x_2^\\prime)^2 = (x_1^2 (x_1^\\prime)^2 + 2 x_1 x_1^\\prime x_2 x_2^\\prime + x_2^2 (x_2^\\prime)^2) = [x_1^2, \\sqrt{2} x_1 x_2, x_2^2] \\cdot [x_1^{\\prime 2}, \\sqrt{2} x_1^\\prime x_2^\\prime, x_2^{\\prime 2}] = \\phi(x)^\\mathrm{T} \\phi(x^\\prime)\n",
    "$$\n",
    "$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad x^\\prime = \\begin{pmatrix} x_1^\\prime \\\\ x_2^\\prime \\end{pmatrix}$$\n",
    "\n",
    "There are several kernel functions, which can be used in combination with a Gaussian process regression model. \n",
    "For simplicity, we will use the [`pairwise_kernels`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html) as part of the scikit-learn package. In the following, we see different functions $f$ drawn from the Gaussian prior $\\mathcal{N}(\\mathbf{f} \\mid \\mathbf{0}, \\mathbf{K})$, where the Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is computed through the radial basis function (RBF) kernel, also known as Gaussian kernel, defined as:\n",
    "\n",
    "_Answer:_ <br/>\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}^\\prime \\rVert^2)\n",
    "$$\n",
    "If $\\gamma = \\sigma^{-2}$, then the kernel is known as the Gaussian kernel of variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037e06dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19775417f6c84fd6ba9da18eb95c818e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='gamma', max=10.0, min=0.01), Output()), _dom_classes…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_prior_functions(gamma):\n",
    "    # Create samples (instances).\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "\n",
    "    # Compute Gram matrix `K` using RBF kernel\n",
    "    # with `gamma`.\n",
    "    K = pairwise_kernels(X, metric='rbf', gamma=gamma)\n",
    "\n",
    "    # Draw 5 function values `f` ~ N(0, K).\n",
    "    f = np.random.multivariate_normal(np.zeros_like(x_axis), K, 5)\n",
    "\n",
    "    # Plot drawn samples `f`.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_axis, f.T)\n",
    "    plt.title(f'Prior functions with gamma={gamma}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.ylim(-4.2, 4.2)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "interactive(\n",
    "    visualize_prior_functions, \n",
    "    gamma=FloatSlider(value=1, min=0.01, max=10), \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e8faa8b",
   "metadata": {},
   "source": [
    "The key results defining a Gaussian process are given through the mean and variance of the conditional distribution $p(y_{N+1} \\mid \\mathcal{D}_N)$. The corresponding formulas are given in the following:\n",
    "\n",
    "_Answer:_<br/>\n",
    "$$\n",
    "\\mu(\\mathbf{x}_{N+1}| \\mathcal{D}_N) = \\mathbf{k}^\\mathrm{T} \\mathbf{C}_N^{-1} \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\rho(\\mathbf{x}_{N+1}| \\mathcal{D}_N) = c - \\mathbf{k}^\\mathrm{T} \\mathbf{C}_N^{-1} \\mathbf{k}\n",
    "$$\n",
    "\n",
    "With this knowledge, we implement the class [`GaussianProcessRegression`](../e2ml/models/_gaussian_process_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage.\n",
    "Once, the implementation has been completed, we check its validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3fb0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fafd0294f54732bda797e34e48a8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='noise', max=10.0), FloatSlider(value=0.01, descripti…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.models import GaussianProcessRegression\n",
    "\n",
    "def objective_function(x, noise, random_state=42):\n",
    "    # noise = np.random.normal(loc=0, scale=noise, size=len(x)) if noise > 0 else 0\n",
    "    # Keep the same noise for each run.\n",
    "    r = np.random.RandomState(random_state)\n",
    "    noise = r.normal(loc=0, scale=noise, size=len(x)) if noise > 0 else 0\n",
    "    return (np.sin(2 * np.pi * x)**6.0 * x) + noise\n",
    "\n",
    "def generate_data(noise, train_ratio, random_state=42):\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "    n_train_samples = int(train_ratio * len(X))\n",
    "    #train_idx = np.random.choice(np.arange(len(X)), replace=False, size=n_train_samples)\n",
    "    # Keep the same training samples for each run.\n",
    "    r = np.random.RandomState(random_state)\n",
    "    train_idx = r.choice(np.arange(len(X)), replace=False, size=n_train_samples)\n",
    "    X_train = X[train_idx]\n",
    "    y_train = objective_function(X_train.ravel(), noise=noise, random_state=random_state)\n",
    "    f = objective_function(x_axis, noise=0, random_state=random_state)\n",
    "    return x_axis, X, f, X_train, y_train\n",
    "\n",
    "def visualize_gpr_predictions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.5, random_state=42, metric='rbf'):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio, random_state)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'metric' : metric} if metric == 'linear' else {'gamma': gamma, 'metric': metric}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, return_std=True)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b', label=\"$\\sigma(x)$\")\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.show()\n",
    "\n",
    "interactive(\n",
    "    visualize_gpr_predictions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.01, max=2, tooltip='1/beta'),\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.01, min=0.01, max=1, step=0.01),\n",
    "    random_state=IntSlider(value=42, min=0, max=100),\n",
    "    kernal=Dropdown(value='rbf', options=['rbf', 'linear'])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "289b78d5",
   "metadata": {},
   "source": [
    "### **2. Acquisition Functions** <a class=\"anchor\" id=\"af\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "200ba74c",
   "metadata": {},
   "source": [
    "Three popular acquisition functions are:\n",
    "- probability improvement defined as:\n",
    "   _Answer:_ <br/>\n",
    "   $$\n",
    "   \\alpha_{PI}(\\mathbf{x}) = \\Phi(z)\n",
    "   $$\n",
    "   where $z = \\frac{\\mu(\\mathbf{x}) - f(\\mathbf{x}^+) - \\xi}{\\sigma(\\mathbf{x})}$, $\\Phi$ is the cumulative distribution function of the standard normal distribution, and $\\xi$ is a hyperparameter.\n",
    "\n",
    "- expected improvement defined as:\n",
    "   _Answer:_ <br/>\n",
    "   $$\n",
    "   \\alpha_{EI}(\\mathbf{x}) = \\sigma(\\mathbf{x}) \\left( z \\Phi(z) + \\phi(z) \\right)\n",
    "   $$\n",
    "   where $z = \\frac{\\mu(\\mathbf{x}) - f(\\mathbf{x}^+) - \\xi}{\\sigma(\\mathbf{x})}$, $\\Phi$ is the cumulative distribution function of the standard normal distribution, and $\\phi$ is the probability density function of the standard normal distribution.\n",
    "\n",
    "- and upper confidence bound defined as:\n",
    "   _Answer:_ <br/>\n",
    "   $$\n",
    "   \\alpha_{UCB}(\\mathbf{x}) = \\mu(\\mathbf{x}) + \\kappa \\sigma(\\mathbf{x})\n",
    "   $$\n",
    "   where $\\kappa$ is a hyperparameter.\n",
    "\n",
    "#### Question:\n",
    "2. (a) What is the task of an acquisition function?\n",
    "\n",
    "   TODO\n",
    "   \n",
    "With this knowledge, we implement the functions \n",
    "\n",
    "- [`acquisition_pi`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- [`acquisition_ei`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- and [`acquisition_ucb`](../e2ml/experimentation/_bayesian_optimization.py)\n",
    "\n",
    "in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage.\n",
    "\n",
    "Once, the implementations have been completed, we check their validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd67ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fce319ef7ff48e9a53d351c9a651a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='noise', max=2.0), FloatSlider(value=0.01, descriptio…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.experimentation import acquisition_pi, acquisition_ei, acquisition_ucb\n",
    "\n",
    "def visualize_acquisition_functions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.1, kappa=1., random_state=42):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio, random_state)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, return_std=True)\n",
    "    \n",
    "    # Determin `tau`.\n",
    "    tau = np.max(y_train)\n",
    "    \n",
    "    # Compute PI scores as `pi_scores`.\n",
    "    pi_scores = acquisition_pi(means, stds, tau)\n",
    "    \n",
    "    # Compute EI scores as `ei_scores`.\n",
    "    ei_scores = acquisition_ei(means, stds, tau)\n",
    "    \n",
    "    # Compute UCB scores as `ucb_scores` with `kappa`.\n",
    "    ucb_scores = acquisition_ucb(means, stds, kappa)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.plot(x_axis, pi_scores, label='PI', c='g')\n",
    "    plt.plot(x_axis, ei_scores, label='EI', c='g', ls='--')\n",
    "    plt.plot(x_axis, ucb_scores, label='UCB', c='g', ls=':')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b', label=\"$\\sigma(x)$\")\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_acquisition_functions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=2),\n",
    "    beta=FloatSlider(value=0.01, min=0.01, max=2),\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.01, min=0.01, max=1, step=0.01),\n",
    "    kappa=FloatSlider(value=1, min=0.0, max=3),\n",
    "    random_state=IntSlider(value=42, min=0, max=100)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dce9ae3",
   "metadata": {},
   "source": [
    "### **3. Bayesian Optimization** <a class=\"anchor\" id=\"bo\"></a>\n",
    "\n",
    "Since we have implemented the Gaussian process as surrogate probabilistic model and possible acquisition functions, we can perform Bayesian optimization. For this purpose, we implement the function [`perform_bayesian_optimization`](../e2ml/experimentation/_bayesian_optimization.py) in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage. Once, the implementation has been completed, we test it on a simple one-dimensional artificial data set through visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d32f0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d5ecfec92f474eaba0da58cfd0f731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=50.0, description='gamma', min=0.01), FloatSlider(value=0.1, descripti…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.experimentation import perform_bayesian_optimization\n",
    "\n",
    "\n",
    "def visualize_bayesian_optimization(gamma=50, noise=0.05, beta=0.05, n_evals=100,\n",
    "                                    n_random_init=5, acquisition_function='pi', random_state=42):\n",
    "    # Generate data and true objective values.\n",
    "    x_axis = np.linspace(-1, 1, 1000)\n",
    "    X_cand = x_axis.reshape(-1, 1)\n",
    "    f = objective_function(x_axis, 0, random_state)\n",
    "\n",
    "    # Partially initialize objective_function.\n",
    "    obj_func = partial(objective_function, noise=noise)\n",
    "\n",
    "    # Create Gaussian process model.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "\n",
    "    # Perform Bayesian optimization with given parameters\n",
    "    # to obtain `X_acquired` and `y_acquired`.\n",
    "    X_acquired, y_acquired = perform_bayesian_optimization(\n",
    "        X_cand, gpr, acquisition_function, obj_func, n_evals, n_random_init, random_state)\n",
    "\n",
    "    # Fit Gaussian process as `gpr` on acquired data.\n",
    "    gpr.fit(X_acquired, y_acquired)\n",
    "\n",
    "    # Predict `means` and `stds` on `X_cand`.\n",
    "    means, stds = gpr.predict(X_cand, return_std=True)\n",
    "\n",
    "    # Visualize Bayesian optimization process.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.scatter(X_acquired.ravel(), y_acquired, s=20,\n",
    "                c='k', label='acquired samples')\n",
    "    for i in range(n_evals):\n",
    "        plt.text(X_acquired[i], y_acquired[i],\n",
    "                 str(i+1), color=\"k\", fontsize=12)\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.title(\n",
    "        'Bayesian optimization with numbers indicating acquisition order', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive(\n",
    "    visualize_bayesian_optimization,\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.01, max=2),\n",
    "    n_evals=IntSlider(value=1, min=1, max=70),\n",
    "    n_random_init=IntSlider(value=5, min=1, max=40),\n",
    "    acquisition_function=Dropdown(options=['pi', 'ei', 'ucb'], value='pi'),\n",
    "    random_state=IntSlider(value=42, min=0, max=100)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
