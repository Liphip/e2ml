{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e83585",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance Measures  I\n",
    "\n",
    "In this notebook, we will implement **performance measures** for evaluating and comparing classifiers in machine learning. \n",
    "\n",
    "At the start, we will implement a function for computing *confusion matrices*.\n",
    "\n",
    "It serves as a basis for computing the subsequent performance measures with a multi- or single-class focus.\n",
    "\n",
    "Finally, we will compare the implemented performance measures using a simple exemplary classification task.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Confusion Matrix](#confusion-matrix)\n",
    "2. [Performances Measures with a Multi-class Focus](#multi-class)\n",
    "3. [Performance Measures with a Single-class Focus](#single-class)\n",
    "4. [Comparison of Performance Measures](#comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6486326",
   "metadata": {},
   "source": [
    "### **1. Confusion Matrix** <a class=\"anchor\" id=\"confusion-matrix\"></a>\n",
    "\n",
    "The confusion matrix $\\mathbf{C}_\\mathcal{T}(h) \\in \\mathbb{N}^{|\\mathcal{Y}| \\times |\\mathcal{Y}|}$ is a table or matrix that is commonly used to evaluate the performance of a\n",
    "classifier $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$. It summarizes the predictions made by the classifier $h$ on a test set $\\mathcal{T} \\subset \\mathcal{X} \\times \\mathcal{Y}$. The (unormalized) entries of the confusion matrix are defined as:\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "$$\n",
    "C_{\\mathcal{T}}^{ij}(h) = \\sum_{(\\mathbf{x}, y) \\in \\mathcal{T}} \\delta\\left(y=i \\wedge h(\\mathbf{x})=j\\right).\n",
    "$$\n",
    "\n",
    "END SOLUTION\n",
    "\n",
    "There exist other variants of a confusion matrix, where the entries of the confusion matrix are normalized row-wise, column-wise, or by the total sum of entries. We implement the function [`confusion_matrix`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import confusion_matrix\n",
    "\n",
    "# Check ranges of class labels.\n",
    "y_1 = [0, -1, 2]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of invalid values.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of invalid values.'\n",
    "\n",
    "# Check type of class labels.\n",
    "y_1 = [\"hello\", \"new\", \"test\"]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a TypeError because of invalid value types.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a TypeError because of invalid value types.'\n",
    "\n",
    "# Check unequal array lengths.\n",
    "y_1 = [0, -1, 2, 3]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# BEGIN SOLUTION\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [1, 0, 1, 0]\n",
    "C_true = [[1, 1], [1, 1]]\n",
    "C = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "np.testing.assert_array_equal(C_true, C)\n",
    "C_true = [[1, 1, 0], [1, 1, 0], [0, 0, 0]]\n",
    "C = confusion_matrix(y_true=y_true, y_pred=y_pred, n_classes=3)\n",
    "np.testing.assert_array_equal(C_true, C)\n",
    "C_true = np.full((2, 2), fill_value=0.25)\n",
    "C = confusion_matrix(y_true=y_true, y_pred=y_pred, n_classes=2, normalize=\"all\")\n",
    "np.testing.assert_array_equal(C_true, C)\n",
    "C_true = np.full((2, 2), fill_value=0.5)\n",
    "C = confusion_matrix(y_true=y_true, y_pred=y_pred, n_classes=2, normalize=\"true\")\n",
    "np.testing.assert_array_equal(C_true, C)\n",
    "C = confusion_matrix(y_true=y_true, y_pred=y_pred, n_classes=2, normalize=\"pred\")\n",
    "np.testing.assert_array_equal(C_true, C)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45055cc",
   "metadata": {},
   "source": [
    "### **2. Performance Measures with a Multi-class Focus** <a class=\"anchor\" id=\"multi-class\"></a>\n",
    "\n",
    "The accuracy $\\mathrm{ACC}_\\mathcal{T}(h) \\in [0, 1]$ of a classifier $h$ on a test set $\\mathcal{T}$ is one of the most know performance measures and can be computed  as the complement of the empirical risk $R_\\mathcal{T}(h)$ or using the confusion matrix $\\mathbf{C}_{\\mathcal{T}}(h)$ as follows:\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "$$\n",
    "\\mathrm{ACC}_{\\mathcal{T}}(h) = 1 - R_\\mathcal{T}(h) = \\frac{\\sum_{y \\in \\mathcal{Y}}C^{yy}_{\\mathcal{T}}(h)}{\\sum_{i \\in \\mathcal{Y}}\\sum_{j \\in \\mathcal{Y}}C^{i j}_{\\mathcal{T}}(h)}.\n",
    "$$\n",
    "\n",
    "END SOLUTION\n",
    "\n",
    "We implement the function [`accuracy`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import accuracy\n",
    "\n",
    "# Check unequal array lengths.\n",
    "y_1 = [0, -1, 2, 3]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    print(accuracy(y_true=y_1, y_pred=y_2))\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "check = False\n",
    "try:\n",
    "    accuracy(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# BEGIN SOLUTION\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [1, 0, 1, 0]\n",
    "acc_true = 0.5\n",
    "assert acc_true == accuracy(y_true=y_true, y_pred=y_pred)\n",
    "y_true = [\"hello\", \"test\", \"test\", \"hello\"]\n",
    "y_pred = [\"hello\", \"test\", \"test\", \"test\"]\n",
    "acc_true = 0.75\n",
    "assert acc_true == accuracy(y_true=y_true, y_pred=y_pred)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ceed79",
   "metadata": {},
   "source": [
    "#### **Question:**\n",
    "2. (a) What are limitations of the accuracy as performance measure?\n",
    "\n",
    "   BEGIN SOLUTION\n",
    "   \n",
    "   Limitations of accuracy are:\n",
    "   - lack of information conveyed by these measures on the varying degree of importance on\n",
    "the class-specific performance,\n",
    "   - inability to convey meaningful information in the case of skewed class distribution,\n",
    "   - inability to distinguish the importance of errors across different classes with unequal misclassification costs.\n",
    "   \n",
    "   END SOLUTION\n",
    "\n",
    "Cohenâ€™s $\\kappa$ represents a more realistic estimate of classifier effectiveness, which is the proportion of labels that the classifier gets right over and above chance agreement. We can compute this performance measure according to:\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "$$\n",
    "\\kappa_\\mathcal{T}(h) = \\frac{p^o_\\mathcal{T}(h) - p^e_\\mathcal{T}(h)}{1 - p^e_\\mathcal{T}(h)}, \n",
    "$$\n",
    "\n",
    "where $p^o_\\mathcal{T}(h)$ is the relative observed agreement among classifier and true labels, while $p^e_\\mathcal{T}(h)$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of classifier/true labels randomly assigning each class.\n",
    "\n",
    "END SOLUTION\n",
    "\n",
    "We implement the function [`cohen_kappa_score`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import cohen_kappa\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# BEGIN SOLUTION\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [1, 0, 1, 0]\n",
    "kappa_true = 0.0\n",
    "assert kappa_true == cohen_kappa(y_true=y_true, y_pred=y_pred)\n",
    "y_true = [1, 1, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 1, 1]\n",
    "kappa_true = 0.0\n",
    "assert kappa_true == cohen_kappa(y_true=y_true, y_pred=y_pred)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd40eb9",
   "metadata": {},
   "source": [
    "### **3. Performance Measures with a Single-class Focus** <a class=\"anchor\" id=\"single-class\"></a>\n",
    "\n",
    "The F measure combines precision and recall in a score by computing the weighted harmonic mean of both. For any $\\alpha \\in \\mathbb{R}_{>0}$, the F measure can be given as:\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "$$\n",
    "F^{\\alpha}_{\\mathcal{T}}(h) = \\frac{(1+\\alpha)\\left(\\mathrm{Prec}_\\mathcal{T}(h) \\cdot \\mathrm{Rec}_\\mathcal{T}(h)\\right)}{\\alpha \\cdot \\mathrm{Prec}_\\mathcal{T}(h) + \\mathrm{Rec}_{\\mathcal{T}}(h)}.\n",
    "$$\n",
    "\n",
    "END SOLUTION\n",
    "\n",
    "The F1 measure or balance F measure weights the recall and precision of the classifier evenly via $\\alpha=1$. The macro F1 measure is an extension toward multi-class problems. Its idea is to compute the F1 score for each class and then taking the arithmetic mean of these scores.\n",
    "\n",
    "We implement the function [`macro_f1_measure`](../e2ml/evaluation/_performance_metrics.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import macro_f1_measure\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# BEGIN SOLUTION\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [1, 0, 1, 0]\n",
    "macro_f1_true = 0.5\n",
    "assert macro_f1_true == macro_f1_measure(y_true=y_true, y_pred=y_pred)\n",
    "y_true = [1, 1, 1, 1, 0, 2, 2, 2, 2, 3]\n",
    "y_pred = [1, 1, 1, 1, 0, 2, 2, 2, 2, 0]\n",
    "macro_f1_true = 2./3.\n",
    "assert macro_f1_true == macro_f1_measure(y_true=y_true, y_pred=y_pred)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dbf14",
   "metadata": {},
   "source": [
    "### **4. Comparison of Performance Measures** <a class=\"anchor\" id=\"comparison\"></a>\n",
    "In the following, we perform an exemplary evaluation study to compare the performance measures accuracy, Cohen's kappa, and macro F1. Therefore, we fit a logistic regression model on a synthetic data set and compute the corresponding measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate classification dataset.\n",
    "X, y = make_blobs(n_samples=[20, 50, 400], random_state=0)\n",
    "\n",
    "# Visualize the dataset.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y) # <-- SOLUTION\n",
    "plt.show() # <-- SOLUTION\n",
    "\n",
    "# Split the dataset into 80% training data and 20% test data.\n",
    "# BEGIN SOLUTION\n",
    "indices = np.arange(len(X))\n",
    "size = int(0.8 * len(indices))\n",
    "test = np.random.RandomState(0).choice(indices, replace=False, size=size)\n",
    "train = np.setdiff1d(indices, test)\n",
    "# END SOLUTION\n",
    "\n",
    "# Fit a logistic regression model on the training data.\n",
    "lr = LogisticRegression(max_iter=2000, random_state=0) # <- SOLUTION\n",
    "lr.fit(X[train], y[train]) # <- SOLUTION\n",
    "\n",
    "# Evaluate and print the three performance measures on the training and test set.\n",
    "# BEGIN SOLUTION\n",
    "y_pred = lr.predict(X)\n",
    "print(f\"Training accuracy: {accuracy(y[train], y_pred[train])}\")\n",
    "print(f\"Test accuracy: {accuracy(y[test], y_pred[test])}\")\n",
    "print(f\"Training Cohen's kappa: {cohen_kappa(y[train], y_pred[train])}\")\n",
    "print(f\"Test Cohen's kappa: {cohen_kappa(y[test], y_pred[test])}\")\n",
    "print(f\"Training macro F1 measure: {macro_f1_measure(y[train], y_pred[train])}\")\n",
    "print(f\"Test macro F1 measure: {macro_f1_measure(y[test], y_pred[test])}\")\n",
    "# END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
