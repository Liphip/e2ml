{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c5f71d",
   "metadata": {},
   "source": [
    "# Foundations of Machine Learning\n",
    "\n",
    "In this notebook, we will learn about the basics of machine learning. \n",
    "\n",
    "At the start, we will implement various **loss and risk** estimation functions.\n",
    "\n",
    "Subsequently, we will implement a **binary logistic regression** (BLR) model according to the [scikit-learn guidelines](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "Further, we will introduce **nonlinear basis functions** as a prerequisite for nonlinear decision boundaries when using BLR models.\n",
    "\n",
    "Finally, we apply BLR models to an artificial data set to investigate the **under- and overfitting** issue.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Loss Functions and Notions of Risk](#loss-functions-and-notions-of-risk)\n",
    "2. [Binary Logistic Regression](#binary-logistic-regression)\n",
    "3. [Nonlinear Basis Functions](#basis-functions)\n",
    "4. [Under- and Overfitting](#under-overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a714745d-6c84-452f-b796-681088b60161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from math import isclose\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2e7e46",
   "metadata": {},
   "source": [
    "### **1. Loss Functions and Notions of Risk** <a class=\"anchor\" id=\"loss-functions-and-notions-of-risk\"></a>\n",
    "\n",
    "The most popular loss function in classification is the zero-one loss function $L_{0/1}$, which is given by\n",
    "\n",
    "_Answer:_\n",
    "$L_{0/1}(y, \\hat{y}) = \\begin{cases} 0 & \\text{if } y = \\hat{y} \\\\ 1 & \\text{if } y \\neq \\hat{y} \\end{cases}$\n",
    "where $y$ is the true label and $\\hat{y}$ is the estimated label.\n",
    "\n",
    "#### **Question:**\n",
    "1. (a) What are the reasons that the zero-one loss function $L_{0/1}$ is typically used for assessing a classifier but not for training?\n",
    "\n",
    "   _Answer:_\n",
    "   The zero-one loss function is not differentiable, which makes it unsuitable for training a classifier. It also is non-convex, which makes it difficult to find a global minimum during training. And not continuous, which makes it difficult to use gradient descent for training.\n",
    "   \n",
    "Alternative loss functions have been proposed that are suitable for optimizing a classifier during training. One of such functions is the *binary cross entropy* (BCE) loss function $L_\\mathrm{BCE}$, which is given by:\n",
    "\n",
    "_Answer:_\n",
    "$L_\\mathrm{BCE}(p, {p^\\prime}) = -p \\ln({p^\\prime}) + (1 - p) \\ln(1 - {p^\\prime})$\n",
    "where $p$ is the true probability and ${p^\\prime}$ is the estimated probability.\n",
    "\n",
    "#### **Question:**\n",
    "1. (b) Is the BCE loss function convex with respect to the estimated probability? Prove your answer.\n",
    "\n",
    "   *Remark: A function $f$ is convex in an interval $\\mathcal{I}$, if $\\forall x \\in \\mathcal{I}: f^{\\prime\\prime}(x) \\geq 0$.*\n",
    "\n",
    "   _Answer:_\n",
    "   By applying the chain rule, we get the following derivatives:\n",
    "   $f({p^\\prime}) = -p \\ln({p^\\prime}) + (1 - p) \\ln(1 - {p^\\prime})$ \n",
    "   \n",
    "   $f^{\\prime}({p^\\prime}) = \\frac{1 - p}{1 - {p^\\prime}} - \\frac{p}{{p^\\prime}} = -\\frac{{p^\\prime} - p}{({p^\\prime} - 1) p}$\n",
    "\n",
    "   $f^{\\prime\\prime}({p^\\prime}) = \\frac{1-p}{{(1 - {p^\\prime})}^2} + \\frac{p}{{p^\\prime}^2}$\n",
    "\n",
    "   since $p \\in [0, 1]$ and ${p^\\prime} \\in (0, 1)$, we get:\n",
    "   $\\forall {p^\\prime} \\in (0, 1): f^{\\prime\\prime}({p^\\prime}) \\geq 0$.\n",
    "   Thus, the BCE loss function is convex with respect to the estimated probability.\n",
    "   \n",
    "For a given loss function $L$ and a dataset $\\mathcal{D}$, the empirical risk $R_{\\mathcal{D}}(h)$ of a classifier $h$ is given by:\n",
    "\n",
    "_Answer:_\n",
    "$R_{\\mathcal{D}}(h) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}} L(y, h(\\mathbf{x}))$\n",
    "where $|\\mathcal{D}|$ is the number of samples in the dataset $\\mathcal{D}$.\n",
    "\n",
    "With this knowledge, we implement the empirical risk for the zero-one loss function $L_{0/1}$ and the BCE loss function $L_{\\mathrm{BCE}}$ in the Python file [`e2ml.evaluation._loss_functions`](../e2ml/evaluation/_loss_functions.py). Afterwards, we check our code by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8377f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import zero_one_loss, binary_cross_entropy_loss\n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0, 0, 1, 0]\n",
    "check = zero_one_loss(y_true, y_pred) == 0.2\n",
    "assert check, 'The empirical risk must be 0.2 for this data set.' \n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0.9, 0.1, 0.5, 0.5]\n",
    "check = isclose(binary_cross_entropy_loss(y_true, y_pred), 0.319, abs_tol=0.001)\n",
    "assert check, 'The empirical risk must be around 0.319 for this data set.' \n",
    "\n",
    "y_true_list = [[2], [-1], [0.5], [0.5]]\n",
    "y_pred_list = [[0.5], [0.5], [2], [-1]]\n",
    "for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "    check = False\n",
    "    try:\n",
    "        binary_cross_entropy_loss(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        check = True\n",
    "    assert check, 'There must be a ValueError because of invalid values.' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca7473c8",
   "metadata": {},
   "source": [
    "### **2. Binary Logistic Regression** <a class=\"anchor\" id=\"binary-logistic-regression\"></a>\n",
    "\n",
    "*Binary logistic regression* (BLR) is a space of classifiers optimizing the BCE loss function. Instead of a class label, a BLR model with weights $\\mathbf{w} \\in \\mathbb{R}^M$ and $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ as a vector of $M$ basis functions estimates the conditional probability according to:\n",
    "\n",
    "\n",
    "_Answer:_ \n",
    "$h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x})) = p(y = 1 | \\boldsymbol{\\phi}(\\mathbf{x}); \\mathbf{w}) = \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})) = \\frac{1}{1 + \\exp(-\\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{w})}$.\n",
    "\n",
    "For a *regularized risk minimization* (RRM) algorithm, we train a BLR model by optimizing\n",
    "\n",
    "_Answer:_\n",
    "#TODO\n",
    "\n",
    "For optimization, we could use a gradient descent approach:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right),$$\n",
    "\n",
    "where $\\nabla$ denotes the gradient, $t$ the current optimization step, and $\\eta \\in \\mathbb{R}_{>0}$ the learning rate defining the step size in each optimization step. The initial weights $\\mathbf{w}_0$ can be chosen randomly and the subsequent optimization steps can be repeated until a stopping criterion is reached, e.g., maximum number of steps. The gradient $\\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right)$ with respect to the weights $\\mathbf{w}$ is then given by:\n",
    "\n",
    "_Answer:_\n",
    "    $R_\\mathcal{D}(h_\\mathbf{w}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}} L_\\mathrm{BCE}(y, h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x})))$ \n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $J(h_\\mathbf{w}) = \\frac{\\lambda}{2 |\\mathcal{D}|} \\sum_{m=1}^M w_m^2$.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    With \n",
    "    <br/>\n",
    "    $L_\\mathrm{BCE}(p, {p^\\prime}) = -p \\ln({p^\\prime}) + (1 - p) \\ln(1 - {p^\\prime})$\n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $G = -y \\ln(h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x}))) - (1 - y) \\ln(1 - h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x})))$\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Than we can write\n",
    "    <br/>\n",
    "    $\\frac{\\partial G}{\\partial W} = \\frac{\\partial G}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\mathbf{w}}$\n",
    "    <br/>\n",
    "    with \n",
    "    <br/>\n",
    "    $h = h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x}))$\n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $z = \\boldsymbol{\\phi}(\\mathbf{x})$.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    We then get\n",
    "    <br/>\n",
    "    $\\frac{\\partial G}{\\partial h} = \\frac{1 - y}{1 - h} - \\frac{y}{h}$\n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $\\frac{\\partial h}{\\partial z} = (1 + \\exp(z))^{-1} \\cdot \\frac{\\partial}{\\partial z} = \\frac{- (1 + \\exp(-z))}{(1 + \\exp(-z))^2} = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = \\frac{1}{1 + \\exp(-z)} \\cdot (\\frac{1 + \\exp(-z)}{1 + \\exp(-z)} - \\frac{1}{1 + \\exp(-z)}) = h \\cdot (\\frac{1 + \\exp(-z)}{1 + \\exp(-z)} - h) = h \\cdot (1 - h)$\n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $\\frac{\\partial z}{\\partial \\mathbf{w}} = \\boldsymbol{\\phi}(\\mathbf{x})_\\mathbf{w}^\\top \\cdot \\frac{\\partial}{\\partial w} = \\boldsymbol{\\phi}(\\mathbf{x})^\\top$.\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Finally, we get\n",
    "    <br/>\n",
    "    $\\frac{\\partial}{\\partial \\mathbf{w}} R_\\mathcal{D}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}} ((h_\\mathbf{w}(\\boldsymbol{\\phi}(\\mathbf{x})) - y) \\cdot \\boldsymbol{\\phi}(\\mathbf{x})$\n",
    "    <br/>\n",
    "    and\n",
    "    <br/>\n",
    "    $\\frac{\\partial}{\\partial \\mathbf{w}} J(h_\\mathbf{w}) = \\frac{\\lambda}{|\\mathcal{D}|} w$.\n",
    "\n",
    "With this knowledge, we implement the class [`BinaryLogisticRegression`](../e2ml/models/_binary_logistic_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage. However, there we use [L-BFGS-B](https://en.wikipedia.org/wiki/Limited-memory_BFGS) as a so-called quasi-Netwon algorithm, which is an advancement of the standard gradient descent algorithm. Once, the implementation has been completed, we check its validity on a simple two-dimensional artificial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a27f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from e2ml.models import BinaryLogisticRegression\n",
    "\n",
    "# Create linearly separable dataset.\n",
    "X, y = make_blobs(n_samples=100, centers=[[-1, -1], [1, 1]], cluster_std=0.1, random_state=0)\n",
    "\n",
    "# Fit `BinaryLogisticRegression` with `maxiter=1` and `lmbda=0` on the artificial data set.\n",
    "binary_logistic_regression = BinaryLogisticRegression(maxiter=1, lmbda=0)\n",
    "binary_logistic_regression.fit(X, y)\n",
    "\n",
    "# Make predictions using the fitted `BinaryLogisticRegression` model on the training set `X`.\n",
    "y_pred = binary_logistic_regression.predict(X)\n",
    "\n",
    "# Compute empirical risk as `risk` with the zero-one loss function on the training set `X`.\n",
    "risk = zero_one_loss(y, y_pred)\n",
    "\n",
    "assert risk == 0, 'The classifier must reach an accuracy of 1.0 for this data set.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d5f6295",
   "metadata": {},
   "source": [
    "### **3. Nonlinear Basis Functions** <a class=\"anchor\" id=\"basis-functions\"></a>\n",
    "\n",
    "For certain learning tasks, it is useful to add complexity to a machine learning model by considering\n",
    "nonlinear transformations of the samples. \n",
    "\n",
    "**Definition 3.12** <font color='red'>**Nonlinear Basis Functions**</font> \n",
    "\n",
    "Nonlinear basis functions $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ nonlinearly transform a $D$-dimensional instance $\\mathbf{x} \\in \\mathbb{R}^D$ to an $M$-dimensional features space $\\mathbb{R}^M$ according to:\n",
    "\n",
    "$$\\boldsymbol{\\phi}(\\mathbf{x}) = \\begin{bmatrix} \\phi_1(\\mathbf{x}) \\\\ \\vdots \\\\ \\phi_M(\\mathbf{x}) \\end{bmatrix}.$$\n",
    "\n",
    "**Remark:** With nonlinear basis functions, a (generalized) linear machine learning model, e.g., BLR, can output nonlinear outputs, e.g., decision boundaries.\n",
    "\n",
    "**Examples:** Two popular types of nonlinear basis functions are: \n",
    "- *multivariate polynomials* with a user-defined maximum degree, e.g., for a degree of two and a two-dimensional instance, we get\n",
    "\n",
    "  $$\\boldsymbol{\\phi}(\\mathbf{x}) = \\boldsymbol{\\phi}\\left(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1 \\cdot x_2 \\\\ x_1^2 \\\\ x_2^2 \\end{bmatrix},$$\n",
    "  \n",
    "- and *Gaussian basis functions* of the form\n",
    "\n",
    "  $$\\phi_j(\\mathbf{x}) = \\exp\\left(\\frac{-||\\mathbf{x} - \\boldsymbol{\\mu}_j||_2^2}{2 \\sigma_j^2}\\right),$$\n",
    "\n",
    "  where $\\boldsymbol{\\mu} \\in \\mathbb{R}^D$ controls the position of the $j$-th basis function and the parameter $\\sigma_j \\in \\mathbb{R}_>0$ its spatial extension.\n",
    "  \n",
    "In the following, we exemplary illustrate the use of Gaussian basis functions. For this purpose, we want to define two basis functions such that the transformed instances define a linearly separable training set, i.e., it should be possible to draw a single straight line separating the blue from the red instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5155a49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4a2cbad1314001bb596e09c08e2a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu_11', max=10.0, min=-10.0), FloatSlider(value=0.0,…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def visualize_gaussian_basis_functions(mu_11, mu_12, mu_21, mu_22, sigma_1, sigma_2):\n",
    "    \"\"\"\n",
    "    Visualizes the application of Gaussian basis functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu_11 : float\n",
    "        First value of mu_1 of the first basis function.\n",
    "    mu_12 : float\n",
    "        Second value of mu_1 of the first basis function.\n",
    "    mu_21 : float\n",
    "        First value of mu_2 of the second basis function.\n",
    "    mu_22 : float\n",
    "        Second value of mu_2 of the second basis function.\n",
    "    sigma_1 : positive float\n",
    "        Spatial extension of the first basis function.\n",
    "    sigma_2 : positive float\n",
    "        Spatial extension of the second basis function.\n",
    "    \"\"\"\n",
    "    X, y = make_blobs(n_samples=50, centers=4, random_state=7, cluster_std=0.75)\n",
    "    y %= 2\n",
    "    \n",
    "    # Define array of `mus`.\n",
    "    mus = np.array([[mu_11, mu_12], [mu_21, mu_22]])\n",
    "    \n",
    "    # Define array of `sigmas`.\n",
    "    sigmas = np.array([sigma_1, sigma_2])\n",
    "    \n",
    "    # Compute transformation via Gaussian basis functions.\n",
    "    Phi = np.exp(-euclidean_distances(X, mus) ** 2 / (2 * sigmas ** 2))\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.scatter(mus[:, 0], mus[:, 1], c=\"green\", s=120, marker=\"x\", label=\"$(\\mu_1, \\mu_2)^\\mathrm{T}$\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(Phi[:, 0], Phi[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.xlabel(\"$\\phi_1$\")\n",
    "    plt.ylabel(\"$\\phi_2$\")\n",
    "    plt.show()\n",
    "\n",
    "interactive(\n",
    "    visualize_gaussian_basis_functions, \n",
    "    mu_11=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_12=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_21=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_22=FloatSlider(value=0, min=-10, max=10),\n",
    "    sigma_1=FloatSlider(value=1, min=0.1, max=10),\n",
    "    sigma_2=FloatSlider(value=1, min=0.1, max=10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58147427",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "3. (a) What is a suitable parametrization of the above basis functions for obtaining a linearly separable transformation?\n",
    "\n",
    "   TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "595067bd",
   "metadata": {},
   "source": [
    "### **4. Under- and Overfitting** <a class=\"anchor\" id=\"under-overfitting\"></a>\n",
    "\n",
    "For our two-dimensional data set, we want to visualize the BLR models estimated decision boundaries including the estimated class-membership probabilities.\n",
    "For this purpose, we implement the function [`plot_decision_boundary`](../e2ml/evaluation/_visualization.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "\n",
    "Given this function, we create an interactive visualization in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5c3d40",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3b209aed84facba4d698e37b0f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, description='maxiter', max=10000), FloatSlider(value=0.0, descript…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.evaluation import plot_decision_boundary\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def visualize_blr(maxiter, lmbda, degree, train_ratio, random_state):\n",
    "    \"\"\"\n",
    "    Visualize decision boundary and estimated conditional class probabilities for a BLR model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    maxiter : positive int\n",
    "        Number of maximum optimization iterations.\n",
    "    lmbda : non-negative float\n",
    "        Regularization rate used by the RRM algorithm.\n",
    "    degree : non-negative int\n",
    "        Maximum degree of the multivariate polynomial.\n",
    "    train_ratio : float in (0, 1)\n",
    "        Ratio of training samples.\n",
    "    \"\"\"\n",
    "    # Generate dataset.\n",
    "    X, y = make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        flip_y=0.25,\n",
    "        random_state=100,\n",
    "        scale=0.1,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "    \n",
    "    # Get boundaries of the feature space.\n",
    "    bound=[[X[:, 0].min()-0.2, X[:, 1].min()-0.2], [X[:, 0].max()+0.2, X[:, 1].max()+0.2]]\n",
    "    \n",
    "    # Randomly split train and test data by creating the index arrays `train_idx` and `test_idx`.\n",
    "    train_idx = np.random.choice(\n",
    "        np.arange(X.shape[0]),\n",
    "        size=int(train_ratio * X.shape[0]),\n",
    "        replace=False,\n",
    "    )\n",
    "    test_idx = np.setdiff1d(np.arange(X.shape[0]), train_idx)\n",
    "    \n",
    "    # Create polynomial feature transformation `poly` with `degree`.\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # Create `blr` model with `n_optimization_steps` and `lmbda`.\n",
    "    blr = BinaryLogisticRegression(maxiter=maxiter, lmbda=lmbda)\n",
    "    \n",
    "    # Create pipeline.\n",
    "    clf = make_pipeline(poly, blr)\n",
    "    \n",
    "    # Fit `BinaryLogisticRegression` as `clf` on training data.\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "    \n",
    "    # Compute zero-one loss on train and test data, i.e., `train_error` and `test_error`.\n",
    "    train_error = zero_one_loss(y[train_idx], clf.predict(X[train_idx]))\n",
    "    test_error = zero_one_loss(y[test_idx], clf.predict(X[test_idx]))\n",
    "    \n",
    "    # Plot results.\n",
    "    plot_decision_boundary(clf=clf, bound=bound)\n",
    "    plt.scatter(\n",
    "        X[train_idx, 0],\n",
    "        X[train_idx, 1],\n",
    "        marker='o',\n",
    "        label='Training Samples',\n",
    "        facecolors='none',\n",
    "        edgecolors='k',\n",
    "        linewidth=4, \n",
    "        s=60\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40)\n",
    "    plt.title(f'Train Error: {round(train_error, 3)}, Test Error: {round(test_error, 3)}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_blr, \n",
    "    maxiter=IntSlider(value=1000, min=0, max=10000), \n",
    "    eta=FloatSlider(value=1, min=0, max=10), \n",
    "    lmbda=FloatSlider(value=0.0, min=0.0, max=10, step=0.001),\n",
    "    degree=IntSlider(value=1, min=1, max=10), \n",
    "    train_ratio=FloatSlider(value=0.5, min=0.02, max=0.98, step=0.01),\n",
    "    random_state=IntSlider(value=0, min=0, max=10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a7363d",
   "metadata": {},
   "source": [
    "The interactive visualization allows us to study the effect of the two parameters `lmbda` and `degree`. In particular, we can study the phonemes of under- and overfitting to answer the following questions:\n",
    "\n",
    "#### Questions:\n",
    "4. (a) How does the regularization rate `lmbda` affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   _Answer:_\n",
    "   The regularization rate `lmbda` controls the trade-off between the model's complexity and the training error. A high regularization rate leads to a simple model with a low training error, but a high generalization error. In contrast, a low regularization rate leads to a complex model with a high training error, but a low generalization error. In the extreme case, a regularization rate of zero leads to a model that perfectly fits the training data, but generalizes poorly to unseen data. This is called overfitting. In contrast, a regularization rate that is too high leads to a model that is too simple to fit the training data well. This is called underfitting.\n",
    "   \n",
    "  (b) How does the `degree` of the polynomial affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   _Answer:_\n",
    "   The `degree` of the polynomial controls the model's complexity. A high degree leads to a complex model with a high training error, but a low generalization error. In contrast, a low degree leads to a simple model with a low training error, but a high generalization error. In the extreme case, a degree of zero leads to a model that perfectly fits the training data, but generalizes poorly to unseen data. This is called overfitting. In contrast, a degree that is too high leads to a model that is too simple to fit the training data well. This is called underfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
