{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DN0bwKsgEdRN"
   },
   "source": [
    "# Performance Measures II\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn about visual performance measures like ROC curves and Lift Charts and information theory based performance measure KL Divergence.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [ROC Analysis](#roc_analysis)\n",
    "2. [Lift Charts](#lift_charts)\n",
    "3. [KL Divergence](#kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.Receiver Operating Characteristic (ROC) Analysis** <a class=\"anchor\" id=\"roc_analysis\"></a>\n",
    "\n",
    "In this section, we implement ROC curve.\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classification model. It illustrates the trade-off between the true positive rate (TPR) and the false positive rate (FPR) at various classification thresholds.\n",
    "\n",
    "The ROC curve is created by plotting the TPR against the FPR as the classification threshold changes. Each point on the curve represents a different threshold setting. The closer the curve is to the top-left corner, the better the model's performance, as it indicates higher TPR and lower FPR.\n",
    "\n",
    "To generate an ROC curve, typically we follow these steps:\n",
    "\n",
    "1. Train a binary classification model on a labeled dataset.\n",
    "2. Obtain the predicted probabilities or scores for each instance in the dataset.\n",
    "3. Vary the classification threshold from 0 to 1.\n",
    "4. For each threshold, calculate the corresponding TPR and FPR based on the predicted probabilities and true labels.\n",
    "5. Plot the obtained TPR-FPR pairs to create the ROC curve.\n",
    "\n",
    "\n",
    "Given below is a simple, but rather inefficient algorithm for computing ROC-curves. \n",
    "The implementation below assumes that ``labels`` is a contain the (numerical) true class label and ``scores`` contain the assigned scores from the classifier (each for the respective instance), while `l` is the class we consider to be the positive one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_inefficient( labels, l, scores ):\n",
    "    score_min, score_max = min(scores), max(scores)\n",
    "    zs = sorted(scores)\n",
    "    incr = min( [ y1-y2 for y1, y2 in zip(zs[1:], zs[:-1])] )\n",
    "    zs = np.array(zs)\n",
    "    \n",
    "    T = len(labels)\n",
    "    P = sum([1 for a in labels if a == l])\n",
    "    N = T-P\n",
    "    \n",
    "    out=[]\n",
    "    if incr != 0:\n",
    "        for t in list(np.arange(score_min, score_max+incr, incr)):\n",
    "            fp, tp = 0, 0\n",
    "            for i in range(len(labels)):\n",
    "                if scores[i] >= t:\n",
    "                    if labels[i] == l:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "            out.append((fp/N, tp/P))\n",
    "    else:\n",
    "        t = zs[0]\n",
    "        fp, tp = 0, 0\n",
    "        for i in range(len(labels)):\n",
    "            if scores[i] >= t:\n",
    "                if labels[i] == l:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "        out.append((fp/N, tp/P))\n",
    "    \n",
    "    return np.unique(np.array(out), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "1. (a) Why is the above function inefficient? Implement an efficient alternative function (roc_curve) to generate roc curve.\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "Sorting the scores: The function sorts the scores list using the sorted() function, which has a time complexity of O(n log n), where n is the length of the list. Sorting the scores is unnecessary for calculating the ROC curve, as the curve can be constructed without sorting the scores.\n",
    "\n",
    "Nested loops: The function uses nested loops to iterate over the labels and scores lists, resulting in a time complexity of O(n^2), where n is the length of the lists. This nested loop structure leads to poor performance, especially for large datasets.\n",
    "\n",
    "Redundant calculations: The function calculates the values of P (number of positive samples) and N (number of negative samples) within each iteration of the loop. These calculations are independent of the loop and can be performed outside the loop to avoid redundant computations.\n",
    "\n",
    "Inefficient data structure: The function uses a list (out) to store the false positive rates (fp/N) and true positive rates (tp/P) for each threshold value. Appending elements to a list in each iteration can be slow, especially when the list grows large. A more efficient approach would be to use a NumPy array or preallocate a list with a fixed size to avoid repeated resizing.\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function [`roc_curve`](../e2ml/evaluation/_performance_measures_II.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import roc_curve\n",
    "# Test correct computation for various simple examples.\n",
    "\n",
    "# Example\n",
    "labels = [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
    "x = 1\n",
    "scores = [0.85, 0.65, 0.64, 0.5, 0.45, 0.95, 0.9, 0.8, 0.68, 0.66]\n",
    "roc_curve_result = roc_curve(labels, x, scores)\n",
    "print(roc_curve_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (b) Now we visualize the curve generated by the previously defined inefficient function and new efficient function to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc_curve(points):\n",
    "    tmp = np.vstack(([[0,0]], points[np.lexsort(points.T)], [[1,1]]))\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    \n",
    "    plt.scatter(tmp[:,0], tmp[:,1])\n",
    "    plt.plot(tmp[:,0], tmp[:,1])\n",
    "    \n",
    "    plt.xlabel('False-Positive Rate', fontsize=15)\n",
    "    plt.xticks(np.linspace(0,1,11))\n",
    "    plt.ylabel('True-Positive Rate', fontsize=15)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.title('ROC Curve', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# Using the example from the book:\n",
    "labelsExample = np.array([   0,    1,    0,   1,    1,    0,   1,   0,    1,    0])\n",
    "scoresExample = np.array([0.85, 0.65, 0.64, 0.5, 0.45, 0.95, 0.9, 0.8, 0.68, 0.66])\n",
    "draw_roc_curve(roc_curve_inefficient(labelsExample, 0, scoresExample))\n",
    "draw_roc_curve(roc_curve(labelsExample, 0, scoresExample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (c) Implement a function that computes the Are Under Curve (AUC) using the values returned by the ROC Curve. \n",
    "\n",
    "    The area under the ROC curve (AUC) is a commonly used metric to evaluate the overall performance of a classification model. A perfect classifier has an AUC of 1, while a random or poor classifier has an AUC close to 0. The AUC provides a single value that summarizes the model's performance across all possible classification thresholds.\n",
    "\n",
    "    AUC is the area of the polygon defined by the points returned by [`roc_curve`](../e2ml/evaluation/_performance_measures_II.py) together with the point $(0,0)$, $(1,0)$ and $(1,1)$.\n",
    "\n",
    "We implement the function [`roc_auc`](../e2ml/evaluation/_performance_measures_II.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for the points of the ROC curve visualized before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import roc_auc\n",
    "points = [(0.0, 0.0), (0.0, 0.2), (0.2, 0.2),  (0.2, 0.4), (0.2, 0.6), (0.4, 0.6), (0.4, 0.8), (0.6, 0.8), (0.6, 1.0), (0.8,1.0),(1.0, 1.0)]\n",
    "auc = roc_auc(points)\n",
    "print(auc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tyaTEcsYS2bg"
   },
   "source": [
    "### **2. Lift Charts** <a class=\"anchor\" id=\"lift_charts\"></a>\n",
    "\n",
    "In this section, we learn to generate Lift charts.\n",
    "\n",
    "Lift charts follow a similar idea to ROC curves. However, instead of plotting the True-Positive against the False-Positive Rate, Lift Charts plot the actual number of true-positives against the number of samples.\n",
    "\n",
    "We implement the function [`draw_lift_chart`](../e2ml/evaluation/_performance_measures_II.py) from lists of the true class labels and the predicted labels in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity with a simple example. We again assume the class labels to be numerical, with ``pos`` being the class which is considered as positive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import draw_lift_chart\n",
    "\n",
    "predictedExample = np.zeros(labelsExample.shape)\n",
    "predictedExample[scoresExample >= 0.8] = 1    \n",
    "draw_lift_chart(labelsExample, 0, predictedExample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Kullback-Leibler (KL) Divergence** <a class=\"anchor\" id=\"kl_divergence\"></a>\n",
    "\n",
    "KL divergence, short for Kullback-Leibler divergence, is a measure of the difference between two probability distributions. It quantifies how one distribution differs from a second, reference distribution. KL divergence is asymmetric and non-negative, meaning that it is not a true distance metric.\n",
    "\n",
    "The KL divergence between two probability distributions P and Q is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    KL(P || Q) = \\sum{P(x) \\cdot \\log{\\left(\\frac{P(x)}{Q(x)}\\right)}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $P(x)$ and $Q(x)$ are the probability mass or density functions of distributions $P$ and $Q$ respectively.\n",
    "\n",
    "We now implement the function [`kl_divergence`](../e2ml/evaluation/_performance_measures_II.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity with a simple example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import kl_divergence\n",
    "# Example \n",
    "p = np.array([0.2, 0.3, 0.5])\n",
    "q = np.array([0.25, 0.25, 0.5])\n",
    "\n",
    "kl_div = kl_divergence(p, q)\n",
    "print(\"KL divergence:\", kl_div)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise6_E2ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
