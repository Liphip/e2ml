{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistical Hypothesis Testing II\n",
    "\n",
    "In this notebook, we will implement and apply **statistical hypothesis tests** to make inferences about risks of learning algorithms.\n",
    "\n",
    "At the start, we will compare two learning algorithms on one domain via the paired $t$-test.\n",
    "\n",
    "Subsequently, we will compare the two learning algorithm across multiple domains via the Wilcoxon signed-rank test.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Paired $t$-test](#paired-t-test)\n",
    "2. [Wilcoxon Signed-rank Test](#wilcoxon-signed-rank-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Paired $t$-test** <a class=\"anchor\" id=\"paired-t-test\"></a>\n",
    "\n",
    "We implement the function [`t_test_paired`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import t_test_paired\n",
    "r_1 = np.round(stats.norm.rvs(loc=0.1, scale=0.03, size=40, random_state=0), 2)\n",
    "r_2 = np.round(stats.norm.rvs(loc=0.12, scale=0.03, size=40, random_state=1), 2)\n",
    "mu_0 = 0\n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"right-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.9256, 'The p-value must be ca. 0.9256 for the one-sided right-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"left-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.0744, 'The p-value must be ca. 0.0744 for the one-sided left-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"two-sided\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.1487, 'The p-value must be ca. 0.1487 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on the data set breast cancer, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "from e2ml.evaluation import cross_validation, zero_one_loss\n",
    "from e2ml.preprocessing import StandardScaler\n",
    "\n",
    "# Load data.\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Generate folds.\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "n_folds = 10\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices, n_folds=n_folds, y=y, random_state=0\n",
    ")\n",
    "\n",
    "# Setup classifiers and risk lists.\n",
    "gpc = GaussianProcessClassifier(random_state=0)\n",
    "risks_gpc = []\n",
    "svc = SVC(random_state=0)\n",
    "risks_svc = []\n",
    "\n",
    "# Perform 10-fold cross validation.\n",
    "for tr, te in zip(train, test):\n",
    "    scaler = StandardScaler().fit(X[tr])\n",
    "    X_train = scaler.transform(X[tr])\n",
    "    X_test = scaler.transform(X[te])\n",
    "    y_gpc = gpc.fit(X_train, y[tr]).predict(X_test)\n",
    "    risks_gpc.append(zero_one_loss(y[te], y_gpc))\n",
    "    y_svc = svc.fit(X_train, y[tr]).predict(X_test)\n",
    "    risks_svc.append(zero_one_loss(y[te], y_svc))\n",
    "    \n",
    "# Perform right-tail paired t-test.\n",
    "alpha = 0.01\n",
    "t_statistic, p = t_test_paired(sample_data_1=risks_gpc, sample_data_2=risks_svc, mu_0=0, test_type=\"right-tail\")\n",
    "if p <= alpha:\n",
    "    print(\n",
    "        f\"Since p={p} <= {alpha} = alpha, we reject H_0 meaning the risk differences \" + \n",
    "        f\"provide sufficient evidence that the SVC has a lower risk than the GPC on \" +\n",
    "        f\"the breast cancer data set given the significance level alpha={alpha}.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Since p={p} > {alpha} = alpha, we do not reject H_0 meaning the risk differences \" + \n",
    "        f\"do not provide sufficient evidence that the SVC has a lower risk than the GPC on \" +\n",
    "        f\"the breast cancer data set given the significance level alpha={alpha}.\"\n",
    "    )\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "1. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   BEGIN SOLUTION\n",
    "\n",
    "   Possible issues are:\n",
    "   - We compare only the default parameter settings since we not perform a nested cross-validation.\n",
    "   - The risk estimates are likely not independent since the training sets strongly overlap.\n",
    "   - We likely violate the normal distribution assumptions of the paired $t$-test since the differences of zero-one loss may not be normally distributed and the number of samples is quite low.\n",
    "\n",
    "   BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Wilcoxon Signed-rank Test** <a class=\"anchor\" id=\"wilcoxon-signed-rank-test\"></a>\n",
    "\n",
    "We implement the function [`wilcoxon_signed_rank_test`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import wilcoxon_signed_rank_test\n",
    "\n",
    "# Test for exact computation.\n",
    "r_1 = stats.norm.rvs(loc=0.1, scale=0.03, size=10, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=0.15, scale=0.03, size=10, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0244, 'The p-value must be ca. 0.0244 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.9814, 'The p-value must be ca. 0.9814 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0488, 'The p-value must be ca. 0.0488 for the two-sided test.' \n",
    "\n",
    "# Test for approximative computation.\n",
    "r_1 = stats.norm.rvs(loc=2, scale=0.3, size=100, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=2.1, scale=0.3, size=100, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0037, 'The p-value must be ca. 0.0037 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.9963, 'The p-value must be ca. 0.9963 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0075, 'The p-value must be ca. 0.0075 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on ten articially generated data sets, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create 10 articial data sets.\n",
    "data_sets = []\n",
    "n_classes_list = np.arange(2, 12)\n",
    "for n_classes in n_classes_list:\n",
    "    X, y = make_classification(\n",
    "        n_samples=500, n_classes=n_classes, class_sep=2, n_informative=10, random_state=n_classes\n",
    "    )\n",
    "    data_sets.append((X, y))\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "    \n",
    "# Generate folds.\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "n_folds = 10\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices, n_folds=n_folds, y=y, random_state=0\n",
    ")\n",
    "\n",
    "# Setup classifiers and risk lists.\n",
    "risks_gpc = []\n",
    "risks_svc = []\n",
    "gpc = GaussianProcessClassifier(random_state=0)\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "# Perform 10-fold cross validation for each data set.\n",
    "for X, y in data_sets:\n",
    "    risks_gpc.append([])\n",
    "    risks_svc.append([])\n",
    "    for tr, te in zip(train, test):\n",
    "        scaler = StandardScaler().fit(X[tr])\n",
    "        X_train = scaler.transform(X[tr])\n",
    "        X_test = scaler.transform(X[te])\n",
    "        y_gpc = gpc.fit(X_train, y[tr]).predict(X_test)\n",
    "        risks_gpc[-1].append(zero_one_loss(y[te], y_gpc))\n",
    "        y_svc = svc.fit(X_train, y[tr]).predict(X_test)\n",
    "        risks_svc[-1].append(zero_one_loss(y[te], y_svc))\n",
    "\n",
    "# Compute mean risks per data set.\n",
    "risks_gpc = np.mean(risks_gpc, axis=1)\n",
    "risks_svc = np.mean(risks_svc, axis=1)\n",
    "        \n",
    "# Perform right-tail paired t-test.\n",
    "alpha = 0.01\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=risks_gpc, sample_data_2=risks_svc, test_type=\"right-tail\")\n",
    "if p <= alpha:\n",
    "    print(\n",
    "        f\"Since p={p} <= {alpha} = alpha, we reject H_0 meaning the risk differences \" + \n",
    "        f\"provide sufficient evidence that the SVC has a lower risk than the GPC on \" +\n",
    "        f\"the ten articially generated data sets given the significance level alpha={alpha}.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Since p={p} > {alpha} = alpha, we do not reject H_0 meaning the risk differences \" + \n",
    "        f\"do not provide sufficient evidence that the SVC has a lower risk than the GPC on \" +\n",
    "        f\"the ten articially generated data sets given the significance level alpha={alpha}.\"\n",
    "    )\n",
    "    \n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   BEGIN SOLUTION\n",
    "\n",
    "   Possible issues are:\n",
    "   - We compare only the default parameter settings since we not perform a nested cross-validation.\n",
    "   - The risk estimates are likely not independent since the artifical data sets are generated with the same method.\n",
    "\n",
    "   BEGIN SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
