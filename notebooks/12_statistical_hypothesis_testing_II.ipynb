{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistical Hypothesis Testing II\n",
    "\n",
    "In this notebook, we will implement and apply **statistical hypothesis tests** to make inferences about risks of learning algorithms.\n",
    "\n",
    "At the start, we will compare two learning algorithms on one domain via the paired $t$-test.\n",
    "\n",
    "Subsequently, we will compare the two learning algorithm across multiple domains via the Wilcoxon signed-rank test.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Paired $t$-test](#paired-t-test)\n",
    "2. [Wilcoxon Signed-rank Test](#wilcoxon-signed-rank-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Paired $t$-test** <a class=\"anchor\" id=\"paired-t-test\"></a>\n",
    "\n",
    "We implement the function [`t_test_paired`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import t_test_paired\n",
    "r_1 = np.round(stats.norm.rvs(loc=0.1, scale=0.03, size=40, random_state=0), 2)\n",
    "r_2 = np.round(stats.norm.rvs(loc=0.12, scale=0.03, size=40, random_state=1), 2)\n",
    "mu_0 = 0\n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"right-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.9256, 'The p-value must be ca. 0.9256 for the one-sided right-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"left-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.0744, 'The p-value must be ca. 0.0744 for the one-sided left-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"two-sided\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.1487, 'The p-value must be ca. 0.1487 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on the data set breast cancer, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "1. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   TODO\n",
    "   TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Wilcoxon Signed-rank Test** <a class=\"anchor\" id=\"wilcoxon-signed-rank-test\"></a>\n",
    "\n",
    "We implement the function [`wilcoxon_signed_rank_test`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import wilcoxon_signed_rank_test\n",
    "\n",
    "# Test for exact computation.\n",
    "r_1 = stats.norm.rvs(loc=0.1, scale=0.03, size=10, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=0.15, scale=0.03, size=10, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0244, 'The p-value must be ca. 0.0244 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.9814, 'The p-value must be ca. 0.9814 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0488, 'The p-value must be ca. 0.0488 for the two-sided test.' \n",
    "\n",
    "# Test for approximative computation.\n",
    "r_1 = stats.norm.rvs(loc=2, scale=0.3, size=100, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=2.1, scale=0.3, size=100, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0037, 'The p-value must be ca. 0.0037 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.9963, 'The p-value must be ca. 0.9963 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0075, 'The p-value must be ca. 0.0075 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on ten articially generated data sets, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create 10 articial data sets.\n",
    "data_sets = []\n",
    "n_classes_list = np.arange(2, 12)\n",
    "for n_classes in n_classes_list:\n",
    "    X, y = make_classification(\n",
    "        n_samples=500, n_classes=n_classes, class_sep=2, n_informative=10, random_state=n_classes\n",
    "    )\n",
    "    data_sets.append((X, y))\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   TODO\n",
    "   TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
